{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "999c20f9-1f2a-485e-a4a1-be95a81d15fb",
   "metadata": {},
   "source": [
    "In this notebook, we re-run our EDA notebooks on the testing set to create a full dataframe with all the features we created. This is so we can run t-tests on the testing set. PLEASE NOTE we have taken out all of the analysis from this notebook. We are NOT looking at ANY of the analysis contained in this notebook. We are merely re-using some of the code to create the necessary features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1b044e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import re\n",
    "from scipy import stats\n",
    "from scipy.stats import zscore, ttest_ind, f_oneway\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45dd6e24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0.1', 'Unnamed: 0', 'channelDescription', 'channelJoinedDate',\n",
       "       'channelTotalVideos', 'channelTotalViews', 'channelUsername',\n",
       "       'commentsCount', 'date', 'duration', 'id', 'isChannelVerified', 'likes',\n",
       "       'numberOfSubscribers', 'order', 'text', 'title', 'url', 'viewCount',\n",
       "       'likes_per_subscriber', 'comments_per_subscriber',\n",
       "       'views_per_subscriber', 'duration_in_seconds', 'datetime',\n",
       "       'datetime_date'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/new/no_early_dates_30_days_test.csv')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c7fdf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data cleaning\n",
    "df[\"text\"] = df[\"text\"].fillna(\"\")\n",
    "df[\"channelDescription\"] = df[\"channelDescription\"].fillna(\"\")\n",
    "# df = df.drop(\"channelLocation\", axis = 1)\n",
    "df = df.dropna().copy()\n",
    "\n",
    "def convert_duration_to_int(item):\n",
    "    item_as_datetime = datetime.datetime.strptime( item , \"%H:%M:%S\"  ) #converts string to a datetime object\n",
    "    seconds = item_as_datetime.second + 60 * item_as_datetime.minute + 3600 * item_as_datetime.hour\n",
    "    return seconds\n",
    "    \n",
    "df[\"duration_in_seconds\"] = df[\"duration\"].apply(convert_duration_to_int) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1dded60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target variables\n",
    "df[\"likes_per_subscriber\"] = df[\"likes\"]/df[\"numberOfSubscribers\"]\n",
    "df[\"comments_per_subscriber\"] = df[\"commentsCount\"]/df[\"numberOfSubscribers\"]\n",
    "df[\"views_per_subscriber\"] = df[\"viewCount\"]/df[\"numberOfSubscribers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9d47ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eda : notebook 1 analyses \n",
    "# eda_p1 : hashtags in title\n",
    "df = df.copy()\n",
    "\n",
    "df[\"hashtag_indicator\"] = df[\"title\"].str.count(\"#\")\n",
    "\n",
    "# eda_p4 : hashtags in description\n",
    "df[\"ht_desc_ind\"] = df[\"text\"].str.count(\"#\") \n",
    "\n",
    "df[\"any_ht\"] = (df[\"title\"] + df[\"text\"]).str.count(\"#\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b1d1a0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fc5f538",
   "metadata": {},
   "outputs": [],
   "source": [
    "#eda : notebook 2 analyses\n",
    "#eda_p5 : affiliate links and discount codes\n",
    "affiliate_patterns = [\n",
    "    r'aff(iliate)?[ -]?link',\n",
    "    r'ref(erral)?[ -]?link',\n",
    "    r'partner[ -]?link',\n",
    "    r'sponsored[ -]?link',\n",
    "    r'^http(s)?:\\/\\/(www\\.)?(amzn\\.to|amazon\\.[a-z\\.]+\\/[^\\s]+tag=)',\n",
    "    r'go\\.magik\\.ly',\n",
    "    r'liketoknow\\.it',\n",
    "    r'prf\\.hn',\n",
    "    r'shareasale',\n",
    "    r'rewardstyle',\n",
    "    r'linktr\\.ee'\n",
    "]\n",
    "\n",
    "discount_patterns = [\n",
    "    r'disc(ount)?[ -]?code',\n",
    "    r'coupon[ -]?code',\n",
    "    r'promo[ -]?code',\n",
    "    r'save \\d+%',\n",
    "    r'\\d+%[ -]?off',\n",
    "    r'use code[: ][a-z0-9_]+'\n",
    "]\n",
    "\n",
    "\n",
    "business_patterns = [\n",
    "    r'business inquir(y|ies)',\n",
    "    r'collaborations?',\n",
    "    r'sponsorships?',\n",
    "    r'partnerships?',\n",
    "    r'for business',\n",
    "    r'contact(\\s+me)?(\\s+for)?(\\s+business)?'\n",
    "]\n",
    "\n",
    "\n",
    "def has_affiliate_or_discount(text):\n",
    "    if pd.isna(text):\n",
    "        return False\n",
    "\n",
    "    has_affiliate = any(re.search(pattern, text, re.IGNORECASE) for pattern in affiliate_patterns)\n",
    "\n",
    "    has_discount = any(re.search(pattern, text, re.IGNORECASE) for pattern in discount_patterns)\n",
    "\n",
    "    return has_affiliate or has_discount\n",
    "\n",
    "def has_business_inquiry(text):\n",
    "    if pd.isna(text):\n",
    "        return False\n",
    "\n",
    "    return any(re.search(pattern, text, re.IGNORECASE) for pattern in business_patterns)\n",
    "\n",
    "df['has_title_affiliate'] = df['title'].apply(has_affiliate_or_discount)\n",
    "df['has_description_affiliate'] = df['text'].apply(has_affiliate_or_discount)\n",
    "df['has_channel_description_affiliate'] = df['channelDescription'].apply(has_affiliate_or_discount)\n",
    "df['has_any_affiliate'] = df['has_title_affiliate'] | df['has_description_affiliate'] | df['has_channel_description_affiliate']\n",
    "\n",
    "df['has_business_inquiry'] = df['channelDescription'].apply(has_business_inquiry)\n",
    "\n",
    "total_videos = len(df)\n",
    "title_affiliates = df['has_title_affiliate'].sum()\n",
    "description_affiliates = df['has_description_affiliate'].sum()\n",
    "channel_description_affiliates = df['has_channel_description_affiliate'].sum()\n",
    "any_affiliates = df['has_any_affiliate'].sum()\n",
    "business_inquiries = df['has_business_inquiry'].sum()\n",
    "affiliate_percentage = (any_affiliates / total_videos) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2306f1bf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893a27a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d230bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ad7ae12",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78d39837",
   "metadata": {},
   "outputs": [],
   "source": [
    "#eda : notebook 3 analyses\n",
    "#eda_p8 : \n",
    "word_list = []\n",
    "list_to_ignore = [\"for\",\"the\",\"a\",\"me\",\"my\",\"in\",\"for\",\"but\",\"of\",\"this\",\"that\",\"The\",\"with\",\"is\",\"you\",\"&\",\"your\",\"it\",\"do\",\"be\",\"by\",\"so\",\"What\",\"what\",\"With\",\"all\",\"i\",\"I\",\"if\",\"Why\",\"why\"]\n",
    "list_to_ignore = set([word.lower() for word in list_to_ignore])\n",
    "for _, row in df.iterrows():\n",
    "    new_words = row[\"title\"].split()\n",
    "    word_list = word_list + [word.lower() for word in new_words if word not in list_to_ignore]\n",
    "\n",
    "\n",
    "def compare_means(df, thing_to_check_for, thing_to_take_average_of, where_to_look=\"title\"):\n",
    "    print(f\"Comparing means of {thing_to_take_average_of} with and without {thing_to_check_for}.\")\n",
    "    idxs = (df[where_to_look].str.count(thing_to_check_for) > 0)\n",
    "    print(df.loc[idxs, thing_to_take_average_of].mean())\n",
    "    print(df.loc[~idxs, thing_to_take_average_of].mean())\n",
    "\n",
    "def get_mean_with_word(df, thing_to_check_for, thing_to_take_average_of, where_to_look=\"title\"):\n",
    "    idxs = (df[where_to_look].str.count(thing_to_check_for) > 0)\n",
    "    return df.loc[idxs, thing_to_take_average_of].mean()\n",
    "\n",
    "def get_mean_without_word(df, thing_to_check_for, thing_to_take_average_of, where_to_look=\"title\"):\n",
    "    idxs = (df[where_to_look].str.count(thing_to_check_for) == 0)\n",
    "    return df.loc[idxs, thing_to_take_average_of].mean()\n",
    "\n",
    "schema = pd.DataFrame()\n",
    "words =[\"dupe\",\"cheap\",\"drugstore\"]# add more words\n",
    "column_to_take_average_of = \"likes_per_subscriber\"\n",
    "where_to_look_for_word = \"title\"\n",
    "schema[\"word\"] = words\n",
    "# Adds column, containing means of likes_per_subscriber of entries with the substring\n",
    "schema['with_string'] = schema[\"word\"].apply(lambda word : get_mean_with_word(df, word, column_to_take_average_of, where_to_look_for_word))\n",
    "# Adds column, containing means of likes_per_subscriber of entries without the substring\n",
    "schema['without_string'] = schema[\"word\"].apply(lambda word : get_mean_without_word(df, word, column_to_take_average_of, where_to_look_for_word))\n",
    "# Adds column, containing means of likes_per_subscriber of entries without the substring\n",
    "schema['difference'] = schema['with_string']-df[\"likes_per_subscriber\"].mean()\n",
    "\n",
    "#finding diffrence between with and without substring\n",
    "schema = schema.sort_values('difference', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "428d01de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arubi\\AppData\\Local\\Temp\\ipykernel_8604\\3251418085.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df['duration_bin'] = pd.cut(filtered_df['duration_in_seconds'], bins=[0, 30, 60, float('inf')], labels=['Short', 'Medium', 'Long'])\n"
     ]
    }
   ],
   "source": [
    "#eda : notebook 4 analyses\n",
    "#eda_p9 : ad impact\n",
    "# subdf = df[['title', 'text', 'id', 'likes', 'viewCount', 'commentsCount', 'duration_in_seconds']]\n",
    "\n",
    "# remove rows with NaN values\n",
    "# subdf = subdf.dropna()\n",
    "# print rows with 'ad' as a substring in the 'title' column or in the 'text' column\n",
    "# DataFrame for rows where 'title' contains 'ad'\n",
    "df['hasAdinTitle'] = df['title'].str.lower().str.contains('ad|sponsored|collaboration|promo|partner|affiliate|paid|gift', case=False, na=False).astype(int)\n",
    "df['hasAdinText'] = df['text'].str.lower().str.contains('ad|sponsored|collaboration|promo|partner|affiliate|paid|gift', case=False, na=False).astype(int)\n",
    "\n",
    "df_stats = df.groupby(['hasAdinTitle', 'hasAdinText'])[['viewCount', 'likes', 'commentsCount']].agg(['mean', 'median', 'count'])\n",
    "# print(df_stats)\n",
    "\n",
    "#eda_p10 : duration impact on views, likes and comments\n",
    "filtered_df = df[df['datetime_date'] >= '2024-10-15']\n",
    "filtered_df['duration_bin'] = pd.cut(filtered_df['duration_in_seconds'], bins=[0, 30, 60, float('inf')], labels=['Short', 'Medium', 'Long'])\n",
    "\n",
    "# Perform t-tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7655eeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the new target variables\n",
    "df['engagement_per_view'] = (df['likes'] + df['commentsCount']) / df['viewCount']\n",
    "df['views_per_subscriber'] = df['viewCount'] / df['numberOfSubscribers']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d2cc95",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c56cab64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots for eda_p9\n",
    "df_corr = df[['viewCount', 'likes', 'commentsCount', 'hasAdinTitle', 'hasAdinText']]\n",
    "\n",
    "corr_matrix = df_corr.corr(method='pearson')\n",
    "df_corr2 = filtered_df[['duration_in_seconds', 'viewCount', 'likes', 'commentsCount']]\n",
    "\n",
    "pearson_corr = df_corr2.corr(method='pearson')\n",
    "spearman_corr = df_corr2.corr(method='spearman')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4426afcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#eda : notebook 5 analyses\n",
    "#eda_p11 : \n",
    "\n",
    "# Standardizing engagement-related variables\n",
    "engagement_vars = [\"likes\", \"commentsCount\", \"viewCount\", \"numberOfSubscribers\"]\n",
    "\n",
    "# Apply Z-score standardization\n",
    "for var in engagement_vars:\n",
    "    df[f\"{var}_std\"] = zscore(df[var])\n",
    "\n",
    "# I will also compute two engagement metrics:\n",
    "# Engagement per Subscriber (to assess subscriber loyalty, but won't reflect new viewers' interactions)\n",
    "# and Engagement per View (to assess how engaging the content is to viewers, but this can be influenced by algorithms)\n",
    "df[\"Engagement_per_Subscriber\"] = (df[\"likes\"] + df[\"commentsCount\"]) / (df[\"numberOfSubscribers\"] + 1)\n",
    "df[\"Engagement_per_View\"] = (df[\"likes\"] + df[\"commentsCount\"]) / (df[\"viewCount\"] + 1)\n",
    "\n",
    "\n",
    "# Ensure no NaN or infinite values in the engagement metrics\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df.dropna(subset=[\"Engagement_per_Subscriber\", \"Engagement_per_View\"], inplace=True)\n",
    "\n",
    "verified = df[df[\"isChannelVerified\"] == True].copy()\n",
    "non_verified = df[df[\"isChannelVerified\"] == False].copy()\n",
    "\n",
    "# Compute T-tests and Effect Sizes (Cohen's d)\n",
    "t_test_results = { \"Metric\": [], \"T-test p-value\": [], \"Effect Size (Cohen's d)\": []}\n",
    "\n",
    "def cohen_d(x, y):\n",
    "    return (np.mean(x) - np.mean(y)) / np.sqrt((np.var(x, ddof=1) + np.var(y, ddof=1)) / 2)\n",
    "\n",
    "for metric in [\"likes_std\", \"commentsCount_std\", \"viewCount_std\", \"numberOfSubscribers_std\", \"Engagement_per_Subscriber\", \"Engagement_per_View\"]:\n",
    "    ttest_p = ttest_ind(verified[metric], non_verified[metric], nan_policy='omit').pvalue\n",
    "    d_value = cohen_d(verified[metric].dropna(), non_verified[metric].dropna())\n",
    "\n",
    "    t_test_results[\"Metric\"].append(metric)\n",
    "    t_test_results[\"T-test p-value\"].append(ttest_p)\n",
    "    t_test_results[\"Effect Size (Cohen's d)\"].append(d_value)\n",
    "\n",
    "# Convert to DataFrame and display results\n",
    "t_test_df = pd.DataFrame(t_test_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f5648d",
   "metadata": {},
   "source": [
    "# Analysis 4 : Verified ?\n",
    "\n",
    "- > Videos from verified channels have significantly more likes, views, comments, engagement and subscribers than those from non-verified channels\n",
    "- >  Videos from verified channels have higher engagement but lower views/subscriber."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb28116",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea3cdde-9f39-4ad8-8bac-fe380cc15960",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "68d669b7-7a13-435e-82fe-4d915f831f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analysis 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "113f6c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#eda : notebook 6 analyses\n",
    "#eda_p12 : impact of time of day, month\n",
    "df1 = df.copy()\n",
    "\n",
    "df1['datetime'] = pd.to_datetime(df['date'])\n",
    "\n",
    "df1['hour'] = df1['datetime'].dt.hour\n",
    "df1['day_of_week'] = df1['datetime'].dt.dayofweek  # 0 is Monday, 6 is Sunday\n",
    "df1['month'] = df1['datetime'].dt.month\n",
    "df1['year'] = df1['datetime'].dt.year\n",
    "\n",
    "day_names = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "\n",
    "df1['day_name'] = df1['day_of_week'].apply(lambda x: day_names[int(x)] if pd.notnull(x) else None)\n",
    "\n",
    "numeric_columns = ['viewCount', 'likes', 'commentsCount', 'hour', 'day_of_week', 'month', 'year']\n",
    "numeric_df = df1[numeric_columns].copy()\n",
    "\n",
    "df1['engagement_rate'] = (df1['likes'] + df1['commentsCount']) / df1['viewCount'] * 100\n",
    "\n",
    "monthly_metrics = df1.groupby('month').agg({\n",
    "    'engagement_rate': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "monthly_metrics['month_name'] = monthly_metrics['month'].apply(lambda x: month_names[int(x)-1] if pd.notnull(x) else None)\n",
    "hourly_metrics = df1.groupby('hour').agg({\n",
    "    'engagement_rate': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "hourly_metrics['hour_label'] = hourly_metrics['hour'].apply(lambda x: f\"{int(x)}:00\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee59cac8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9bfde179-5be8-4766-8017-7dd05904ccf7",
   "metadata": {},
   "source": [
    "Analysis 7\n",
    "\n",
    "In the following section we will see if mentioning a popular brand has an impact on post performance. \n",
    "We have hand-selected 30 popular beauty brands; 15 of these are skincare brands and 15 of these are makeup brands. This list is subjective and may not be exhaustive but should cover a good number of popular brands. \n",
    "\n",
    "Brands:\n",
    "Makeup:\n",
    "1) Natasha Denona\n",
    "2) Tower 28\n",
    "3) Pat McGrath\n",
    "4) Urban Decay\n",
    "5) ColourPop\n",
    "6) Fenty Beauty\n",
    "7) E.L.F. cosmetics\n",
    "8) Nyx professional makeup\n",
    "9) Essence\n",
    "10) Benefit Cosmetics\n",
    "11) Anastasia Beverly Hills\n",
    "12) Tarte\n",
    "13) Milk Makeup\n",
    "14) Maybelline\n",
    "15) Oden's Eye\n",
    "\n",
    "Skincare:\n",
    "1) The Ordinary\n",
    "2) Beauty of Josean\n",
    "3) Bubble\n",
    "4) Paula's Choice\n",
    "5) Cerave\n",
    "6) Good Molecules\n",
    "7) Cosrx\n",
    "8) Olive Young\n",
    "9) Dennis Grossman\n",
    "10) Skinfix\n",
    "11) Drunk Elephant\n",
    "12) La Roche-Posay\n",
    "13) Supergoop\n",
    "14) Glow Recipe\n",
    "15) Rhode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "63242e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#eda : notebook 7 analyses\n",
    "#eda_p13 : brand impact\n",
    "\n",
    "mentions = [\"natasha denona\", \"natashadenona\", \"denona\", \"tower 28\", \"tower28\", \"pat mcgrath\", \"pmg labs\", \"mcgrath\", \"patmcgrath\"]\n",
    "mentions += [\"urban decay\", \"urbandecay\", \"colourpop\", \"colorpop\", \"colour pop\", \"fenty\", \"e.l.f.\", \"elf\", \"nyx\", \"essence\", \"benefit\"]\n",
    "mentions += [\"anastasia\", \"abh\", \"tarte\", \"milk\", \"maybelline\", \"oden's eye\", \"oden'seye\", \"odenseye\", \"the ordinary\", \"theordinary\"]\n",
    "mentions += [\"beauty of josean\", \"josean\", \"bubble\", \"paula's choice\", \"paula'schoice\", \"paulaschoice\", \"cerave\", \"good molecules\"]\n",
    "mentions += [\"cosrx\", \"olive young\", \"oliveyoung\", \"grossman\", \"skinfix\", \"drunk elephant\", \"drunkelephant\", \"roche-posay\", \"roche posay\", \"rocheposay\"]\n",
    "mentions += [\"supergoop\", \"glow recipe\", \"glowrecipe\", \"rhode\"]\n",
    "text_list = []\n",
    "\n",
    "for item in df[\"text\"]:\n",
    "    x = item if type(item) == str else '' \n",
    "    text_list.append(x)\n",
    "\n",
    "df[\"text\"] = np.array(text_list)\n",
    "df[\"title plus desc\"] = df[\"title\"] + df[\"text\"] #Creating a single column of both title and description so that my for loop works in the next step. \n",
    "\n",
    "popb_list = []\n",
    "for item in df[\"title plus desc\"]:\n",
    "    item = item.lower()\n",
    "\n",
    "    ment_bool = False\n",
    "    for ment in mentions:\n",
    "        if (ment in item):\n",
    "            ment_bool = True\n",
    "\n",
    "    popb_list.append(ment_bool) \n",
    "    \n",
    "df[\"popular_brand\"] = np.array(popb_list) \n",
    "df_yes = df.loc[  df[\"popular_brand\"] == True]\n",
    "df_no = df.loc[  df[\"popular_brand\"] == False]\n",
    "# # Replace whitespaces in column names with underscores\n",
    "# df.columns = df.columns.str.replace(' ', '_')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f7ff14-761a-4297-9ef3-8f3550e9fa9e",
   "metadata": {},
   "source": [
    "Here are my initial thoughts based on the analysis: Mentioning popular brands seems to significantly improve views. It seems to improves likes/comments, but not necessarily significantly. This is quite strange! One reason for this might be because these posts tend to mention popular brands in hashtags, which improve views, even though people overall are not more likely to comment or like on your post just because you mentioned a popular brand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13929869-aec0-46fc-b127-f4086a36566f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a226119-5e69-440e-abbc-00e9992141c4",
   "metadata": {},
   "source": [
    "The above t-test seems to indicate that yes, posts which mention a popular brand tend to have higher views on average.\n",
    "I would recommend a simialar t-test be done on the testing set for our final analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2a3f77e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arubi\\anaconda3\\envs\\erdos_spring_2025\\Lib\\site-packages\\scipy\\integrate\\_quadpack_py.py:1260: IntegrationWarning: The integral is probably divergent, or slowly convergent.\n",
      "  quad_r = quad(f, low, high, args=args, full_output=self.full_output,\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'prime_hours' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 43\u001b[0m\n\u001b[0;32m     35\u001b[0m df_for_tukey[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhour\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_for_tukey[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhour\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)  \u001b[38;5;66;03m# Convert hour to string for the test\u001b[39;00m\n\u001b[0;32m     37\u001b[0m tukey_results \u001b[38;5;241m=\u001b[39m pairwise_tukeyhsd(\n\u001b[0;32m     38\u001b[0m     df_for_tukey[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mengagement_rate\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     39\u001b[0m     df_for_tukey[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhour\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     40\u001b[0m     alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m\n\u001b[0;32m     41\u001b[0m )\n\u001b[1;32m---> 43\u001b[0m prime_engagement \u001b[38;5;241m=\u001b[39m df1[df1[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhour\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misin(\u001b[43mprime_hours\u001b[49m)][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mengagement_rate\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     44\u001b[0m non_prime_engagement \u001b[38;5;241m=\u001b[39m df1[\u001b[38;5;241m~\u001b[39mdf1[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhour\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misin(prime_hours)][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mengagement_rate\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     48\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatetime\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'prime_hours' is not defined"
     ]
    }
   ],
   "source": [
    "#eda : notebook 8 analyses\n",
    "#eda_p14 : prime time\n",
    "\n",
    "df1 = df.copy()\n",
    "\n",
    "df1['datetime'] = pd.to_datetime(df['date'])\n",
    "\n",
    "df1['hour'] = df1['datetime'].dt.hour\n",
    "df1['day_of_week'] = df1['datetime'].dt.dayofweek  # 0 is Monday, 6 is Sunday\n",
    "df1['month'] = df1['datetime'].dt.month\n",
    "df1['year'] = df1['datetime'].dt.year\n",
    "\n",
    "day_names = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "\n",
    "df1['day_name'] = df1['day_of_week'].apply(lambda x: day_names[int(x)] if pd.notnull(x) else None)\n",
    "\n",
    "numeric_columns = ['viewCount', 'likes', 'commentsCount', 'hour', 'day_of_week', 'month', 'year']\n",
    "numeric_df = df1[numeric_columns].copy()\n",
    "\n",
    "df1['engagement_rate'] = (df1['likes'] + df1['commentsCount']) / df1['viewCount'] * 100\n",
    "\n",
    "monthly_metrics = df1.groupby('month').agg({\n",
    "    'engagement_rate': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "monthly_metrics['month_name'] = monthly_metrics['month'].apply(lambda x: month_names[int(x)-1] if pd.notnull(x) else None)\n",
    "# Group data by hour and create lists of engagement rates for each hour\n",
    "hour_groups = [df1[df1['hour'] == hour]['engagement_rate'].values for hour in range(24)]\n",
    "# Remove any empty groups\n",
    "hour_groups = [group for group in hour_groups if len(group) > 0]\n",
    "\n",
    "# 2. Pairwise t-tests\n",
    "df_for_tukey = df1[['hour', 'engagement_rate']].copy()\n",
    "df_for_tukey['hour'] = df_for_tukey['hour'].astype(str)  # Convert hour to string for the test\n",
    "\n",
    "tukey_results = pairwise_tukeyhsd(\n",
    "    df_for_tukey['engagement_rate'],\n",
    "    df_for_tukey['hour'],\n",
    "    alpha=0.05\n",
    ")\n",
    "\n",
    "prime_engagement = df1[df1['hour'].isin(prime_hours)]['engagement_rate']\n",
    "non_prime_engagement = df1[~df1['hour'].isin(prime_hours)]['engagement_rate']\n",
    "\n",
    "\n",
    "\n",
    "df['datetime'] = pd.to_datetime(df['date'])\n",
    "df['hour'] = df['datetime'].dt.hour\n",
    "df[\"prime_hour\"] = df['hour'].isin(prime_hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3911ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot for eda_p14\n",
    "correlation_matrix = numeric_df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5348b3e2-324e-4b8d-b307-2b3b87e66e00",
   "metadata": {},
   "source": [
    "# Analysis 8 - Keywords\n",
    "\n",
    "- > Summary of results: Posts with keywords in the \"korean\", \"speed\", and \"product\" keyword groups seem to perform well with regard to likes/subscriber and views/subscriber, with \"korean\" being the top performer.\n",
    "\n",
    "\n",
    "In the following section, we will check to see which keywords are good to use in the title and/or description to boost performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6039343-be5c-4edc-b974-6d5e1c669e80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4207fa-5421-49b0-af2a-ab1ed82d6950",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I was wondering if there were keywords I wasn't thinking of - just from some bias I have, so instead I search for all words used in every title\n",
    "#And order them by frequency. \n",
    "#I then tried to remove the obvious articles and conjunctions, but somes still come through, maybe some blank space or smthn subtle? \n",
    "\n",
    "word_list = []\n",
    "list_to_ignore = [\"for\",\"the\",\"a\",\"me\",\"my\",\"in\",\"for\",\"but\",\"of\",\"this\",\"that\",\"The\",\"with\",\"is\",\"you\",\"&\",\"your\",\"it\",\"do\",\"be\",\"by\",\"so\",\"What\",\"what\",\"With\",\"all\",\"i\",\"I\",\"if\",\"Why\",\"why\"]\n",
    "list_to_ignore = set([word.lower() for word in list_to_ignore])\n",
    "for _, row in df.iterrows():\n",
    "    new_words = row[\"title\"].split()\n",
    "    word_list = word_list + [word.lower() for word in new_words if word not in list_to_ignore]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7697fd3-bb1a-40a8-b93d-706b5030fe3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#streamlining the \"presence of a substring\" function Rachael was implementing earlier - Jo\n",
    "\n",
    "def compare_means(df, thing_to_check_for, thing_to_take_average_of, where_to_look=\"title\"):\n",
    "    print(f\"Comparing means of {thing_to_take_average_of} with and without {thing_to_check_for}.\")\n",
    "    idxs = (df[where_to_look].str.count(thing_to_check_for) > 0)\n",
    "    print(df.loc[idxs, thing_to_take_average_of].mean())\n",
    "    print(df.loc[~idxs, thing_to_take_average_of].mean())\n",
    "\n",
    "def get_mean_with_word(df, thing_to_check_for, thing_to_take_average_of, where_to_look=\"title\"):\n",
    "    idxs = (df[where_to_look].str.count(thing_to_check_for) > 0)\n",
    "    return df.loc[idxs, thing_to_take_average_of].mean()\n",
    "\n",
    "def get_mean_without_word(df, thing_to_check_for, thing_to_take_average_of, where_to_look=\"title\"):\n",
    "    idxs = (df[where_to_look].str.count(thing_to_check_for) == 0)\n",
    "    return df.loc[idxs, thing_to_take_average_of].mean()\n",
    "\n",
    "# Identify popular video topics/formats and see if videos that cover these topics perform better than average. \n",
    "# For example, one popular video format is \"speed reviews.\"\n",
    "#\"speed reviews\", \"haul\", \"dupe\" , \"GRWM\"  etc\n",
    "\n",
    "\n",
    "# Create a dataframe\n",
    "schema = pd.DataFrame()\n",
    "\n",
    "words =[\"dupe\",\"cheap\",\"drugstore\"]# add more words\n",
    "column_to_take_average_of = \"likes_per_subscriber\"\n",
    "where_to_look_for_word = \"title\"\n",
    "\n",
    "#added color vs colour to see if there was an american bias\n",
    "#try vs tryon, since try includes both tryon, as well as \"trying\" \n",
    "#celebrity names to see if people want to copy signature looks?\n",
    "#asmr, grwm, vlog have secondary content separate from the objective makeup \n",
    "#retail stores to see if people are looking to purchase\n",
    "#ten - trying to see if people like lists? i.e. \"my top ten\"\n",
    "\n",
    "schema[\"word\"] = words\n",
    "\n",
    "\n",
    "# Adds column, containing means of likes_per_subscriber of entries with the substring\n",
    "schema['with_string'] = schema[\"word\"].apply(lambda word : get_mean_with_word(df, word, column_to_take_average_of, where_to_look_for_word))\n",
    "\n",
    "# Adds column, containing means of likes_per_subscriber of entries without the substring\n",
    "schema['without_string'] = schema[\"word\"].apply(lambda word : get_mean_without_word(df, word, column_to_take_average_of, where_to_look_for_word))\n",
    "\n",
    "\n",
    "# Adds column, containing means of likes_per_subscriber of entries without the substring\n",
    "schema['difference'] = schema['with_string']-df[\"likes_per_subscriber\"].mean()\n",
    "\n",
    "\n",
    "#finding diffrence between with and without substring\n",
    "\n",
    "schema = schema.sort_values('difference', ascending=False)\n",
    "\n",
    "# show dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f1a379-5faa-41a4-a1ac-2ccc12d2e4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#keyword groups\n",
    "comparison_list = [\"unbox\",\"haul\",\"review\",\"try\",\"vs\"]\n",
    "skills_list=[\"tip\",\"trick\",\"hack\",\"tutorial\"]\n",
    "skincare_list = [\"skincare\",\"mask\",\"toner\"]#\"sunscreen\",\"acne\",\"clean\",\"snail\",\"serum\"\n",
    "product_list=[\"tint\",\"lipgloss\",\"blush\",\"balm\",\"foundation\",\"lipstick\",\"concealer\",\"eyeshadow\",\"mascara\",\"oil\"] #\n",
    "speed_list=[\"short\",\"speed\",\"quick\",\"fast\",\"routine\"]\n",
    "#adjective_list=[\"color\",\"colour\",\"shade\",\"swatch\"]\n",
    "#brand_noun_list=[\"fenty\",\"dior\"]\n",
    "budget_list=[\"dupe\",\"cheap\",\"drugstore\"] #\"budget\",\"affordable\"\n",
    "self_ref_list=[\"viral\",\"short\",\"popular\",\"fav\",\"best\",\"cute\",\"easy\",\"trend\",\"makeup\",\"beauty\"]\n",
    "acronym_list=[\"grwm\",\"ootd\",\"asmr\"]\n",
    "kniche_list=[\"kbeauty\",\"korean\"]\n",
    "#texture_list=[\"jelly\",\"gel\",\"matte\",\"glitter\"]\n",
    "#season=[\"summer\",\"winter\",\"fall\",\"valentine\",\"spring\"]\n",
    "\n",
    "def contains_substring(text, substring_list):\n",
    "    for substring in substring_list:\n",
    "        if substring.lower() in text.lower():\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def contains_products(text):\n",
    "    return 1 * contains_substring(text, product_list)\n",
    "def contains_budget(text):\n",
    "    return 1 * contains_substring(text, budget_list)\n",
    "def contains_self_ref(text):\n",
    "    return 1 * contains_substring(text, self_ref_list)\n",
    "def contains_acronym(text):\n",
    "    return 1 * contains_substring(text, acronym_list)\n",
    "def contains_kniche(text):\n",
    "    return 1 * contains_substring(text, kniche_list)\n",
    "def contains_speed(text):\n",
    "    return 1 * contains_substring(text, speed_list)\n",
    "def contains_skills(text):\n",
    "    return 1 * contains_substring(text, skills_list)\n",
    "def contains_comparison(text):\n",
    "    return 1 * contains_substring(text, comparison_list)\n",
    "def contains_skincare(text):\n",
    "    return 1 * contains_substring(text, skincare_list)\n",
    "\n",
    "\n",
    "#skincare and acronym are somewhat poorly performing, but I left them in for now, and we can decide?\n",
    "#the three groups I would recommend are \"comparing_products\",\"skills/teach\",\"speed\" and \"self_ref\". \n",
    "#korean is there because it just does very well, would be hard to ignore its effect but doesnt really fit into a group?\n",
    "#budget is there because...it feels intuitive to mention, just does moderately well\n",
    "\n",
    "df = df.copy()\n",
    "\n",
    "df[\"product\"] = df[\"title\"].apply(contains_products)\n",
    "df[\"budget\"] = df[\"title\"].apply(contains_budget)\n",
    "df[\"self_ref\"] = df[\"title\"].apply(contains_self_ref)\n",
    "df[\"acronym\"] = df[\"title\"].apply(contains_acronym)\n",
    "df[\"korean\"] = df[\"title\"].apply(contains_kniche)\n",
    "df[\"speed\"] = df[\"title\"].apply(contains_speed)\n",
    "df[\"skills/teach\"] = df[\"title\"].apply(contains_skills)\n",
    "df[\"skincare\"] = df[\"title\"].apply(contains_skincare)\n",
    "df[\"comparing_products\"] = df[\"title\"].apply(contains_comparison)\n",
    "\n",
    "#The following value counts check to make sure there is a decent amount of posts from each category\n",
    "\n",
    "keywords = [\"product\", \"skills/teach\", \"speed\", \"comparing_products\", \"self_ref\", \"budget\", \"korean\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90629a5-d362-472e-bf17-0745b8cd4a45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cb9064-e062-41ef-b53f-9a55dab0964c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8467921e-ad82-472f-b258-f77c87397cc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94de8ac-6b44-42c9-a4dd-e00135e4afa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4718de1-227b-4a90-86a4-6a7e1dac97ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"isChannelVerified\", \"any_ht\", \"commentsCount\", \"isChannelVerified\", \"likes\", \"numberOfSubscribers\", \"text\", \"title\", \"viewCount\", \"views_per_subscriber\", \n",
    "            \"duration_in_seconds\", \"date\", \"hashtag_indicator\", \"has_any_affiliate\", \"hasAdinTitle\", \"hasAdinText\", \"Engagement_per_Subscriber\", \n",
    "            \"Engagement_per_View\", \"popular_brand\", \"prime_hour\", \"product\", \"skills/teach\", \"speed\", \"comparing_products\", \"self_ref\", \"budget\", \"korean\"]\n",
    "\n",
    "df[features].to_csv(\"../data/new/no_early_dates_all_features_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9cd24f-cbf4-4582-86ca-658da601e9b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Erdos Institute)",
   "language": "python",
   "name": "erdos_spring_2025"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
