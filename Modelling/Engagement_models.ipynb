{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f93ffa0-b4ff-4b12-81f2-949ebad42546",
   "metadata": {},
   "source": [
    "In this notebook, we will create and compare several models. Our target feature for this notebooks is (Likes+Comments/(Views+1))\n",
    "which we will call \"engagement\". We have added 1 to the denonimator just to ensure we never divide by zero. \n",
    "\n",
    "Please note that we are not really creating a \"model\" of our data. That is, we do not believe our data is fully explained by our features or any features that can be extracted from our data set. This explains the poor fit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "98f96210-9b6b-4589-ae6a-be7b219121e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('../data/new/no_early_dates_all_features_train.csv')\n",
    "features = [\"popular_brand\", \"has_any_affiliate\", \"product\", \"budget\", \"self_ref\", \"korean\", \"speed\", \n",
    "            \"skills/teach\", \"comparing_products\", \"prime_hour\", \"hashtag_indicator\", \"hasAdinTitle\", \"hasAdinText\"]\n",
    "\n",
    "#Create the target column $y$ here \n",
    "\n",
    "df[\"y\"] = (df[\"likes\"] + df[\"commentsCount\"])  / (df[\"viewCount\"] + 1) \n",
    "\n",
    "#get rid of noisy columns\n",
    "df = df[ features + [\"y\"] ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "42cf1fcf-3422-4167-9ed1-d695d3e8c59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import everything\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import root_mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bea5e3ea-744f-4ea3-a082-27bb47cd1ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do an 80-20 Train Test Split Here. Never ever touch the testing set please!\n",
    "\n",
    "df_train, df_test = train_test_split(df, shuffle = True, test_size = .2, random_state = 42) #We can't stratify because we have too many categorical features. I hope this is ok\n",
    "#DO NOT TOUCH THE ABOVE X_TEST VARIABLE FOR ANY REASON\n",
    "\n",
    "#We want a very basic idea of the MSE and other metrics for each model, before we do proper cross-validation. We use a secondary split for this.\n",
    "df_tt, df_ho = train_test_split(df_train, shuffle = True, test_size = .2, random_state = 42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "13d86438-aae7-4433-8644-e26d56daae6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error: 0.029123\n",
      "R-squared: -0.0056\n"
     ]
    }
   ],
   "source": [
    "#Create the baseline model here\n",
    "\n",
    "class BaseMeanModel():\n",
    "    def __init__(self):\n",
    "        self.mean_value = None\n",
    "    \n",
    "    def fit(self, values : pd.Series):\n",
    "        self.mean_value = values.mean()\n",
    "\n",
    "    def predict(self, inputs=None):\n",
    "        if inputs is None:\n",
    "            return self.mean_value\n",
    "        return len(inputs) * [self.mean_value]\n",
    "    \n",
    "    \n",
    "model = BaseMeanModel()\n",
    "model.fit(df_tt[\"y\"])\n",
    "\n",
    "# R2 is negative because training set and the hold out set have different average values\n",
    "y_pred = model.predict(df_ho[features])\n",
    "rmse = root_mean_squared_error(df_ho[\"y\"], y_pred)\n",
    "r2 = r2_score(df_ho[\"y\"], y_pred)\n",
    "print(f\"Root Mean Squared Error: {rmse:.6f}\")\n",
    "print(f\"R-squared: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fc720b8d-d4a5-41ed-8e0c-cce7dfb81885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error: 0.028268\n",
      "R-squared: 0.0526\n"
     ]
    }
   ],
   "source": [
    "#Creat the basic linear regression model here \n",
    "###Fitting the basic linear regression model using the training set and print the summary of the model.\n",
    "model = LinearRegression()\n",
    "model.fit(df_tt[features], df_tt[\"y\"])\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(df_ho[features])\n",
    "rmse = root_mean_squared_error(df_ho[\"y\"], y_pred)\n",
    "r2 = r2_score(df_ho[\"y\"], y_pred)\n",
    "print(f\"Root Mean Squared Error: {rmse:.6f}\")\n",
    "print(f\"R-squared: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3e891559-d14a-47a2-af18-a5363af7e60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Alpha: 0.0001\n",
      "Test MSE: 0.000801, RMSE: 0.028294, MAE: 0.021494\n",
      "R² Score: 0.050788, Explained Variance: 0.055645\n"
     ]
    }
   ],
   "source": [
    "#Create the basic linear regression model here with lasso regression. \n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LassoCV, Lasso\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, explained_variance_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(df_tt[features] )\n",
    "X_test_scaled = scaler.transform( df_ho[features])\n",
    "\n",
    "alpha = 0.0001\n",
    "lasso = Lasso(alpha=alpha, random_state=42, max_iter=10000)\n",
    "\n",
    "#cv_scores = cross_val_score(lasso, X_train_scaled, df_tt[\"y\"], cv=5,scoring='neg_mean_squared_error')\n",
    "#cv_rmse = np.sqrt(-cv_scores.mean())\n",
    "\n",
    "lasso.fit(X_train_scaled, df_tt[\"y\"])\n",
    "\n",
    "y_pred = lasso.predict(X_test_scaled)\n",
    "\n",
    "# Calculate metrics\n",
    "mse = mean_squared_error(df_ho[\"y\"], y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(df_ho[\"y\"], y_pred)\n",
    "r2 = r2_score(df_ho[\"y\"], y_pred)\n",
    "exp_var = explained_variance_score(df_ho[\"y\"], y_pred)\n",
    "\n",
    "\n",
    "print(f\"\\nAlpha: {alpha}\")\n",
    "print(f\"Test MSE: {mse:.6f}, RMSE: {rmse:.6f}, MAE: {mae:.6f}\")\n",
    "print(f\"R² Score: {r2:.6f}, Explained Variance: {exp_var:.6f}\")\n",
    "#print(f\"Cross-validated RMSE: {cv_rmse:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "39edb992-3082-4415-a155-dbd36c360774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 0.000789, RMSE: 0.028086, MAE: 0.021365\n",
      "R² Score: 0.064693, Explained Variance: 0.069416\n"
     ]
    }
   ],
   "source": [
    "#Create a model whose features include all interaction terms\n",
    "pipe = Pipeline([  (\"interaction terms\", PolynomialFeatures(degree = 2, interaction_only = True, include_bias = False) ),\n",
    "                   (\"linear model\", LinearRegression())\n",
    "]) \n",
    "#setting degree = 2 creates all pairwise interaction terms. \n",
    "\n",
    "pipe.fit( df_tt[features], df_tt[\"y\"]) \n",
    "pred = pipe.predict( df_ho[features] )\n",
    "\n",
    "mse = mean_squared_error(df_ho[\"y\"], pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(df_ho[\"y\"], pred)\n",
    "r2 = r2_score(df_ho[\"y\"], pred)\n",
    "exp_var = explained_variance_score(df_ho[\"y\"], pred)\n",
    "\n",
    "\n",
    "print(f\"Test MSE: {mse:.6f}, RMSE: {rmse:.6f}, MAE: {mae:.6f}\")\n",
    "print(f\"R² Score: {r2:.6f}, Explained Variance: {exp_var:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b43452cc-12d0-4ac3-882b-d7942693d0e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE :  0.028043613259332946\n"
     ]
    }
   ],
   "source": [
    "#Create a model with all interaction terms and lasso regression \n",
    "\n",
    "#Create a model with all interaction terms and lasso regression \n",
    "from sklearn.linear_model import Lasso, LassoCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaled_df_tt = scaler.fit_transform( df_tt[features] )\n",
    "scaled_df_ho = scaler.transform( df_ho[features] )\n",
    "\n",
    "# using lasso cv to find the best alpha\n",
    "# lasso = LassoCV(cv=5, random_state=42, max_iter=10000, alphas=np.logspace(-4, 1, 30))\n",
    "# lasso.fit(df_tt[features], df_tt[\"y\"])\n",
    "# pred = lasso.predict(df_ho[features])\n",
    "# print(\"Lasso CV MSE:\", root_mean_squared_error(df_ho[\"y\"], pred))\n",
    "# print(\"Optimal alpha:\", lasso.alpha_)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"interaction terms\", PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)),\n",
    "    (\"lasso\", Lasso(alpha=0.0001, max_iter=10000))\n",
    "])\n",
    "pipe.fit(scaled_df_tt, df_tt[\"y\"])\n",
    "pred = pipe.predict(scaled_df_ho)\n",
    "print(\"MSE : \", root_mean_squared_error(df_ho[\"y\"], pred))\n",
    "\n",
    "# Get lasso coefficients\n",
    "lasso_coeffs = pd.Series(pipe.named_steps['lasso'].coef_, index=pipe.named_steps['interaction terms'].get_feature_names_out(features))\n",
    "lasso_coeffs = lasso_coeffs[lasso_coeffs != 0]\n",
    "# print(lasso_coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2e6b6f89-6217-41e4-a47e-b0f06f1ec11b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.02912292 0.03097684 0.03660978 0.03208654 0.03187867]\n",
      " [0.02826778 0.03054189 0.03612537 0.03144913 0.03141039]\n",
      " [0.02829447 0.03053    0.03612154 0.03145887 0.03140658]\n",
      " [0.02808646 0.02990547 0.03575154 0.03147655 0.03093863]\n",
      " [0.02804361 0.02988082 0.03573833 0.03136356 0.03087301]]\n"
     ]
    }
   ],
   "source": [
    "#Do cross-validation to compare all models \n",
    "\n",
    "#Model 0: Baseline Average\n",
    "#Model 1: Basic Linear Regression Model\n",
    "#Model 2: Linear Regression with Lasso\n",
    "#Model 3: Linear Regression with interaction terms\n",
    "#Model 4: Linear Regression with interactions and lasso\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "num_splits = 5\n",
    "num_models = 5\n",
    "\n",
    "kfold = KFold(num_splits,\n",
    "              random_state = 42,\n",
    "              shuffle=True)\n",
    "\n",
    "rmses = np.zeros((num_models, num_splits))\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(kfold.split(df_train)): \n",
    "\n",
    "    df_tt = df_train.iloc[train_index]\n",
    "    df_ho = df_train.iloc[test_index] \n",
    "\n",
    "    #Model 0: Baseline Average\n",
    "    model = BaseMeanModel()\n",
    "    model.fit(df_tt[\"y\"])\n",
    "    y_pred = model.predict(df_ho[features])\n",
    "    rmses[0,i] = root_mean_squared_error(df_ho[\"y\"], y_pred)\n",
    "\n",
    "    #Model 1: Basic Linear Regression Model\n",
    "    model = LinearRegression()\n",
    "    model.fit(df_tt[features], df_tt[\"y\"])\n",
    "    y_pred = model.predict(df_ho[features])\n",
    "    rmses[1,i] = root_mean_squared_error(df_ho[\"y\"], y_pred)\n",
    "\n",
    "    #Model 2: Linear Regression with Lasso\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(df_tt[features] )\n",
    "    X_test_scaled = scaler.transform( df_ho[features])\n",
    "    alpha = 0.0001\n",
    "    lasso = Lasso(alpha=alpha, random_state=42, max_iter=10000)\n",
    "    lasso.fit(X_train_scaled, df_tt[\"y\"])\n",
    "    y_pred = lasso.predict(X_test_scaled)\n",
    "    rmses[2,i] = root_mean_squared_error(df_ho[\"y\"], y_pred)\n",
    "\n",
    "    #Model 3: Linear Regression with interaction terms\n",
    "    pipe = Pipeline([  (\"interaction terms\", PolynomialFeatures(degree = 2, interaction_only = True, include_bias = False) ),\n",
    "                   (\"linear model\", LinearRegression())\n",
    "                    ]) \n",
    "    pipe.fit( df_tt[features], df_tt[\"y\"]) \n",
    "    y_pred = pipe.predict( df_ho[features] )\n",
    "    rmses[3,i] = root_mean_squared_error(df_ho[\"y\"], y_pred)\n",
    "\n",
    "    #Model 4: Linear Regression with interactions and lasso\n",
    "    scaler = StandardScaler()\n",
    "    scaled_df_tt = scaler.fit_transform( df_tt[features] )\n",
    "    scaled_df_ho = scaler.transform( df_ho[features] )\n",
    "    pipe = Pipeline([\n",
    "    (\"interaction terms\", PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)),\n",
    "    (\"lasso\", Lasso(alpha=0.0001, max_iter=10000))\n",
    "                    ])\n",
    "    pipe.fit(scaled_df_tt, df_tt[\"y\"])\n",
    "    y_pred = pipe.predict(scaled_df_ho)\n",
    "    rmses[4,i] = root_mean_squared_error(df_ho[\"y\"], y_pred)\n",
    "\n",
    "\n",
    "print(rmses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d17df561-474b-41dc-a4be-3659ec40bc0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.03213495, 0.03155891, 0.03156229, 0.03123173, 0.03117987])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmses.mean(axis = 1)\n",
    "#Pretty much as expected, models with the most features (i.e., models that included interaction terms) did the best.\n",
    "#Interestingly, Lasso regression slightly outperformed regular linear regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7dd1bfe5-da0d-4074-969c-8aacea12ba04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note that in the RMSE matrix, each ROW is a different model and each COLUMN is a CV. \n",
    "#In the above we took an average of each ROW. \n",
    "#As a sanity check we look at the full RMSE matrix and see that yes, in each column the first row is the largest number,\n",
    "#the second row is (often) the second largest number, etc. This means there is a consistent pattern as to the performance of our models\n",
    "#and it is not just random noise. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98cb8ba-44b5-4a98-99ac-6df2b4c153d3",
   "metadata": {},
   "source": [
    "---Final Interpretation---\n",
    "\n",
    "Almost as expected, the model with the most features (i.e., the model that includes all interaction terms) performed the best. This could just be because every time a linear model has more features it will perform better, in the sense that it will have a lower rmse. \n",
    "\n",
    "Interestingly, our lasso model slightly outperformed our non-lasso model. \n",
    "\n",
    "We want to take the coefficients of our highest performing model and compare them. This is where our analysis was really headed.\n",
    "Mathematically, the features with the highest coefficients are those that contribute the most to the trend line.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "abbfe974-86a9-4582-96b0-bd6c3b4f5288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "popular_brand                    -0.002639\n",
      "has_any_affiliate                 0.001629\n",
      "product                          -0.001590\n",
      "budget                           -0.000186\n",
      "self_ref                          0.002271\n",
      "                                    ...   \n",
      "prime_hour hasAdinTitle          -0.000164\n",
      "prime_hour hasAdinText            0.000089\n",
      "hashtag_indicator hasAdinTitle   -0.000815\n",
      "hashtag_indicator hasAdinText    -0.001132\n",
      "hasAdinTitle hasAdinText          0.000106\n",
      "Length: 73, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#A final fitting to the whole training data set \n",
    "scaler = StandardScaler()\n",
    "scaled_df = scaler.fit_transform( df[features] )\n",
    "pipe = Pipeline([\n",
    "    (\"interaction terms\", PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)),\n",
    "    (\"lasso\", Lasso(alpha=0.0001, max_iter=10000))\n",
    "                    ])\n",
    "pipe.fit(scaled_df, df[\"y\"])\n",
    "\n",
    "lasso_coeffs = pd.Series(pipe.named_steps['lasso'].coef_, index=pipe.named_steps['interaction terms'].get_feature_names_out(features))\n",
    "sig_lasso_coeffs = lasso_coeffs[lasso_coeffs != 0]\n",
    "print(sig_lasso_coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e03d7661-73cd-480a-8f74-467b86e08949",
   "metadata": {},
   "outputs": [],
   "source": [
    "new = sig_lasso_coeffs.sort_values(key=abs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "80b10feb-2632-410d-8ac8-9b31945b3b66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "product                          -0.001590\n",
       "has_any_affiliate                 0.001629\n",
       "popular_brand skills/teach        0.001649\n",
       "skills/teach hashtag_indicator   -0.001696\n",
       "self_ref speed                   -0.001802\n",
       "prime_hour                        0.001925\n",
       "self_ref                          0.002271\n",
       "product hashtag_indicator         0.002460\n",
       "popular_brand                    -0.002639\n",
       "hashtag_indicator                -0.003277\n",
       "dtype: float64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba936c8-6015-4a72-98e5-4610c5b9c43d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Erdos Institute)",
   "language": "python",
   "name": "erdos_spring_2025"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
