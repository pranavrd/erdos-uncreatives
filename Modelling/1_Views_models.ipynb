{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5e37523-42d1-466e-8490-cb4b6931b933",
   "metadata": {},
   "source": [
    "In this notebook, we will create and compare several models. Our target feature for this notebooks is LOG(VIEWS/SUBSCRIBERS))\n",
    "\n",
    "We are looking at the log to mitigate the effect of viral videos which are extreme outliers. \n",
    "\n",
    "We consider the following models:\n",
    "1) A baseline model that always outputs the average value of the target variable\n",
    "2) A basic linear regression model\n",
    "3) A linear regression model fit with lasso regression\n",
    "4) A model that includes all pairwise interaction terms\n",
    "5) A model that includeds all pairwise interaction terms fit with lasso regression.\n",
    "\n",
    "Almost as expected, the model with the most features (i.e., the model that includes all interaction terms) performed the best. This is this could just be because every time a linear model has more features it will perform better, in the sense that it will have a lower rmse. Interestingly, our lasso model slightly outperformed our non-lasso model. \n",
    "   \n",
    "Please note that we are not really creating a \"model\" of our data. That is, we do not believe our targets are fully explained by our features or any features that can be extracted from our data set. This makes sense - the views and engagement of a youtube short is primarily explained by the video content, which is not part of our data set, though things like the title and description have some effect. This explains the poor fit. However, we hope to understand something about the data from looking at these models. For example, we found that our best model had an $R^2$ score of .0990 and an explained variance score of .0992. These numbers tell us to what extend our targets are explainable with our features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6540735a-9ca7-4f9f-bb48-b0e2bb6d189f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'commentsCount', 'isChannelVerified', 'likes',\n",
       "       'numberOfSubscribers', 'text', 'title', 'viewCount',\n",
       "       'views_per_subscriber', 'duration_in_seconds', 'date',\n",
       "       'hashtag_indicator', 'has_any_affiliate', 'hasAdinTitle', 'hasAdinText',\n",
       "       'Engagement_per_Subscriber', 'Engagement_per_View', 'popular_brand',\n",
       "       'prime_hour', 'product', 'skills/teach', 'speed', 'comparing_products',\n",
       "       'self_ref', 'budget', 'korean'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math \n",
    "\n",
    "df = pd.read_csv('../data/new/no_early_dates_all_features_train.csv')\n",
    "df.columns\n",
    "\n",
    "#PLEASE NOTE THAT THE HASHTAGS COLUMN CURRENTLY HAS THE NUMBER OF HASHTAGS USED, AND IS NOT A CATEGORICAL VARIABLE. \n",
    "#WHEN CONSIDERING INTERACTION TERMS PLEASE ONLY INCLUDE PAIRWISE INTERACTION TERMS. More interaction terms than this would create extremely small and nonexistent categories which we do not want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7c69868-7f1b-4f87-99df-50c03fc443bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Don't run this cell more than once\n",
    "\n",
    "features = [\"popular_brand\", \"has_any_affiliate\", \"product\", \"budget\", \"self_ref\", \"korean\", \"speed\", \n",
    "            \"skills/teach\", \"comparing_products\", \"prime_hour\", \"hashtag_indicator\", \"hasAdinTitle\", \"hasAdinText\"]\n",
    "\n",
    "#Create the target column $y$ here:\n",
    "df[\"y\"] = df[\"views_per_subscriber\"].apply( math.log )\n",
    "\n",
    "#We don't need a lot of the noise columns\n",
    "df = df[ features + [\"y\"] ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "174ee18d-50d9-4f5d-b30b-a78281db3cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import everything\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import root_mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2dff4f7-65bd-4f28-abfc-833d304078b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do an 80-20 Train Test Split Here. Never ever touch the testing set please!\n",
    "cat_features = [\"popular_brand\", \"has_any_affiliate\", \"product\", \"budget\", \"self_ref\", \"acronym\", \"korean\", \"speed\", \"skills/teach\", \"skincare\", \"comparing_products\", \"prime_hour\", \"hasAdinTitle\", \"hasAdinText\"]\n",
    "#The above is just everything except \"hashtags\"\n",
    "\n",
    "df_train, df_test = train_test_split(df, shuffle = True, test_size = .2, random_state = 42) #We can't stratify because we have too many categorical features. I hope this is ok\n",
    "#DO NOT TOUCH THE ABOVE X_TEST VARIABLE FOR ANY REASON\n",
    "\n",
    "#We want a very basic idea of the MSE for each model, before we do proper cross-validation. We use a secondary split for this.\n",
    "df_tt, df_ho = train_test_split(df_train, shuffle = True, test_size = .2, random_state = 42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6428dbc1-d78e-47bb-9159-b42eea6a20d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error: 1.606709\n",
      "R-squared: -0.0002\n"
     ]
    }
   ],
   "source": [
    "#Create the baseline model here\n",
    "\n",
    "class BaseMeanModel():\n",
    "    def __init__(self):\n",
    "        self.mean_value = None\n",
    "    \n",
    "    def fit(self, values : pd.Series):\n",
    "        self.mean_value = values.mean()\n",
    "\n",
    "    def predict(self, inputs=None):\n",
    "        if inputs is None:\n",
    "            return self.mean_value\n",
    "        return len(inputs) * [self.mean_value]\n",
    "    \n",
    "model = BaseMeanModel()\n",
    "model.fit(df_tt[\"y\"])\n",
    "\n",
    "# R2 is negative because training set and the hold out set have different average values\n",
    "y_pred = model.predict(df_ho[features])\n",
    "rmse = root_mean_squared_error(df_ho[\"y\"], y_pred)\n",
    "r2 = r2_score(df_ho[\"y\"], y_pred)\n",
    "print(f\"Root Mean Squared Error: {rmse:.6f}\")\n",
    "print(f\"R-squared: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a9c1ca4-fcb4-4c9c-a0f4-5e8990aab033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (Log Views): 1.530634\n",
      "R-squared: 0.0922\n"
     ]
    }
   ],
   "source": [
    "#Create the basic linear regression model here \n",
    "model = LinearRegression()\n",
    "model.fit(df_tt[features], df_tt[\"y\"])\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(df_ho[features])\n",
    "rmse = root_mean_squared_error(df_ho[\"y\"], y_pred)\n",
    "r2 = r2_score(df_ho[\"y\"], y_pred)\n",
    "print(f\"Root Mean Squared Error (Log Views): {rmse:.6f}\")\n",
    "print(f\"R-squared: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d31581e-3aa9-4b52-b749-9993cf3299f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Alpha: 0.0001\n",
      "Test MSE: 2.342785, RMSE: 1.530616, MAE: 1.187705\n",
      "R² Score: 0.092251, Explained Variance: 0.092339\n"
     ]
    }
   ],
   "source": [
    "#Create the basic linear regression model here with lasso regression. \n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LassoCV, Lasso\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, explained_variance_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(df_tt[features] )\n",
    "X_test_scaled = scaler.transform( df_ho[features])\n",
    "\n",
    "\n",
    "alpha = 0.0001\n",
    "\n",
    "lasso = Lasso(alpha=alpha, random_state=42, max_iter=10000)\n",
    "\n",
    "\n",
    "lasso.fit(X_train_scaled, df_tt[\"y\"])\n",
    "\n",
    "y_pred = lasso.predict(X_test_scaled)\n",
    "\n",
    "# Calculate metrics\n",
    "mse = mean_squared_error(df_ho[\"y\"], y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(df_ho[\"y\"] ,y_pred)\n",
    "r2 = r2_score(df_ho[\"y\"], y_pred)\n",
    "exp_var = explained_variance_score(df_ho[\"y\"], y_pred)\n",
    "\n",
    "print(f\"\\nAlpha: {alpha}\")\n",
    "print(f\"Test MSE: {mse:.6f}, RMSE: {rmse:.6f}, MAE: {mae:.6f}\")\n",
    "print(f\"R² Score: {r2:.6f}, Explained Variance: {exp_var:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28f85933-593b-4710-844a-0a1c7bc5436d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 2.326234, RMSE: 1.525200, MAE: 1.179644\n",
      "R² Score: 0.098663, Explained Variance: 0.098919\n"
     ]
    }
   ],
   "source": [
    "#Create a model whose features include all interaction terms \n",
    "pipe = Pipeline([  (\"interaction terms\", PolynomialFeatures(degree = 2, interaction_only = True, include_bias = False) ),\n",
    "                   (\"linear model\", LinearRegression())\n",
    "]) \n",
    "#setting degree = 2 creates all pairwise interaction terms. \n",
    "\n",
    "pipe.fit( df_tt[features], df_tt[\"y\"]) \n",
    "pred = pipe.predict( df_ho[features] )\n",
    "\n",
    "mse = mean_squared_error(df_ho[\"y\"], pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(df_ho[\"y\"], pred)\n",
    "r2 = r2_score(df_ho[\"y\"], pred)\n",
    "exp_var = explained_variance_score(df_ho[\"y\"], pred)\n",
    "\n",
    "\n",
    "print(f\"Test MSE: {mse:.6f}, RMSE: {rmse:.6f}, MAE: {mae:.6f}\")\n",
    "print(f\"R² Score: {r2:.6f}, Explained Variance: {exp_var:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b12a5133-9cea-407b-b0db-d0b9503d0279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 2.325441, RMSE: 1.524940, MAE: 1.179512\n",
      "R² Score: 0.098971, Explained Variance: 0.099227\n"
     ]
    }
   ],
   "source": [
    "#Create a model with all interaction terms and lasso regression \n",
    "\n",
    "#Create a model with all interaction terms and lasso regression \n",
    "from sklearn.linear_model import Lasso, LassoCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaled_df_tt = scaler.fit_transform( df_tt[features] )\n",
    "scaled_df_ho = scaler.transform( df_ho[features] )\n",
    "\n",
    "# using lasso cv to find the best alpha\n",
    "# lasso = LassoCV(cv=5, random_state=42, max_iter=10000, alphas=np.logspace(-4, 1, 30))\n",
    "# lasso.fit(df_tt[features], df_tt[\"y\"])\n",
    "# pred = lasso.predict(df_ho[features])\n",
    "# print(\"Lasso CV MSE:\", root_mean_squared_error(df_ho[\"y\"], pred))\n",
    "# print(\"Optimal alpha:\", lasso.alpha_)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"interaction terms\", PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)),\n",
    "    (\"lasso\", Lasso(alpha=0.0001, max_iter=10000))\n",
    "])\n",
    "pipe.fit(scaled_df_tt, df_tt[\"y\"])\n",
    "pred = pipe.predict(scaled_df_ho)\n",
    "\n",
    "# Get lasso coefficients\n",
    "lasso_coeffs = pd.Series(pipe.named_steps['lasso'].coef_, index=pipe.named_steps['interaction terms'].get_feature_names_out(features))\n",
    "lasso_coeffs = lasso_coeffs[lasso_coeffs != 0]\n",
    "# print(lasso_coeffs)\n",
    "\n",
    "mse = mean_squared_error(df_ho[\"y\"], pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(df_ho[\"y\"], pred)\n",
    "r2 = r2_score(df_ho[\"y\"], pred)\n",
    "exp_var = explained_variance_score(df_ho[\"y\"], pred)\n",
    "\n",
    "print(f\"Test MSE: {mse:.6f}, RMSE: {rmse:.6f}, MAE: {mae:.6f}\")\n",
    "print(f\"R² Score: {r2:.6f}, Explained Variance: {exp_var:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a48034c-5135-4a52-a6f3-57bb5d03c11b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.6067091  1.62643628 1.64651932 1.67372172 1.59652834]\n",
      " [1.53063439 1.55575122 1.56093855 1.59344457 1.50422675]\n",
      " [1.53061577 1.55576445 1.56094054 1.59343502 1.50424112]\n",
      " [1.5251996  1.52278023 1.54089224 1.58851722 1.4893488 ]\n",
      " [1.52493956 1.52266765 1.54073961 1.58831359 1.48927094]]\n"
     ]
    }
   ],
   "source": [
    "#Do cross-validation to compare all models \n",
    "\n",
    "#Model 0: Baseline Average\n",
    "#Model 1: Basic Linear Regression Model\n",
    "#Model 2: Linear Regression with Lasso\n",
    "#Model 3: Linear Regression with interaction terms\n",
    "#Model 4: Linear Regression with interactions and lasso\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "num_splits = 5\n",
    "num_models = 5\n",
    "\n",
    "kfold = KFold(num_splits,\n",
    "              random_state = 42,\n",
    "              shuffle=True)\n",
    "\n",
    "rmses = np.zeros((num_models, num_splits))\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(kfold.split(df_train)): \n",
    "\n",
    "    df_tt = df_train.iloc[train_index]\n",
    "    df_ho = df_train.iloc[test_index] \n",
    "\n",
    "    #Model 0: Baseline Average\n",
    "    model = BaseMeanModel()\n",
    "    model.fit(df_tt[\"y\"])\n",
    "    y_pred = model.predict(df_ho[features])\n",
    "    rmses[0,i] = root_mean_squared_error(df_ho[\"y\"], y_pred)\n",
    "\n",
    "    #Model 1: Basic Linear Regression Model\n",
    "    model = LinearRegression()\n",
    "    model.fit(df_tt[features], df_tt[\"y\"])\n",
    "    y_pred = model.predict(df_ho[features])\n",
    "    rmses[1,i] = root_mean_squared_error(df_ho[\"y\"], y_pred)\n",
    "\n",
    "    #Model 2: Linear Regression with Lasso\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(df_tt[features] )\n",
    "    X_test_scaled = scaler.transform( df_ho[features])\n",
    "    alpha = 0.0001\n",
    "    lasso = Lasso(alpha=alpha, random_state=42, max_iter=10000)\n",
    "    lasso.fit(X_train_scaled, df_tt[\"y\"])\n",
    "    y_pred = lasso.predict(X_test_scaled)\n",
    "    rmses[2,i] = root_mean_squared_error(df_ho[\"y\"], y_pred)\n",
    "\n",
    "    #Model 3: Linear Regression with interaction terms\n",
    "    pipe = Pipeline([  (\"interaction terms\", PolynomialFeatures(degree = 2, interaction_only = True, include_bias = False) ),\n",
    "                   (\"linear model\", LinearRegression())\n",
    "                    ]) \n",
    "    pipe.fit( df_tt[features], df_tt[\"y\"]) \n",
    "    y_pred = pipe.predict( df_ho[features] )\n",
    "    rmses[3,i] = root_mean_squared_error(df_ho[\"y\"], y_pred)\n",
    "\n",
    "    #Model 4: Linear Regression with interactions and lasso\n",
    "    scaler = StandardScaler()\n",
    "    scaled_df_tt = scaler.fit_transform( df_tt[features] )\n",
    "    scaled_df_ho = scaler.transform( df_ho[features] )\n",
    "    pipe = Pipeline([\n",
    "    (\"interaction terms\", PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)),\n",
    "    (\"lasso\", Lasso(alpha=0.0001, max_iter=10000))\n",
    "                    ])\n",
    "    pipe.fit(scaled_df_tt, df_tt[\"y\"])\n",
    "    y_pred = pipe.predict(scaled_df_ho)\n",
    "    rmses[4,i] = root_mean_squared_error(df_ho[\"y\"], y_pred)\n",
    "\n",
    "print(rmses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bcd3249e-7178-409f-95c0-f0fcca1fa21a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.62998295, 1.5489991 , 1.54899938, 1.53334762, 1.53318627])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmses.mean(axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90674a6-d45c-4c46-979f-d08de1620da6",
   "metadata": {},
   "source": [
    "#Final interpretation. We'll look at coefficients for our best model and compare. \n",
    "\n",
    "---Final Interpretation---\n",
    "\n",
    "Almost as expected, the model with the most features (i.e., the model that includes all interaction terms) performed the best. This is this could just be because every time a linear model has more features it will perform better, in the sense that it will have a lower rmse. \n",
    "\n",
    "Interestingly, our lasso model slightly outperformed our non-lasso model. \n",
    "\n",
    "We want to take the coefficients of our highest performing model and compare them. This is where our analysis was really headed.\n",
    "Mathematically, the features with the highest coefficients are those that contribute the most to the trend line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66079140-9136-4386-9b0b-2f90b5746a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "popular_brand                     0.057680\n",
      "has_any_affiliate                -0.149690\n",
      "product                           0.128541\n",
      "budget                            0.038506\n",
      "self_ref                         -0.078659\n",
      "                                    ...   \n",
      "prime_hour hasAdinTitle          -0.026373\n",
      "prime_hour hasAdinText           -0.038124\n",
      "hashtag_indicator hasAdinTitle   -0.046976\n",
      "hashtag_indicator hasAdinText    -0.111520\n",
      "hasAdinTitle hasAdinText         -0.017357\n",
      "Length: 91, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#A final fitting to the whole training data set \n",
    "scaler = StandardScaler()\n",
    "scaled_df = scaler.fit_transform( df[features] )\n",
    "pipe = Pipeline([\n",
    "    (\"interaction terms\", PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)),\n",
    "    (\"lasso\", Lasso(alpha=0.0001, max_iter=10000))\n",
    "                    ])\n",
    "pipe.fit(scaled_df, df[\"y\"])\n",
    "\n",
    "lasso_coeffs = pd.Series(pipe.named_steps['lasso'].coef_, index=pipe.named_steps['interaction terms'].get_feature_names_out(features))\n",
    "sig_lasso_coeffs = lasso_coeffs[lasso_coeffs != 0]\n",
    "print(sig_lasso_coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "861bc90d-63d0-4f13-80cd-f8143db619a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "budget korean                      0.000166\n",
       "comparing_products hasAdinTitle    0.000383\n",
       "popular_brand hasAdinTitle         0.001020\n",
       "comparing_products prime_hour      0.001604\n",
       "popular_brand product              0.002075\n",
       "                                     ...   \n",
       "has_any_affiliate                 -0.149690\n",
       "hasAdinText                       -0.164558\n",
       "has_any_affiliate speed            0.173918\n",
       "korean                             0.253364\n",
       "hashtag_indicator                  0.368265\n",
       "Length: 91, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sig_lasso_coeffs.sort_values(key=abs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7adeb2ca-5146-40bf-aae1-94b492af799d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "comparing_products               0.091376\n",
       "has_any_affiliate budget         0.102200\n",
       "hashtag_indicator hasAdinText   -0.111520\n",
       "skills/teach hasAdinText         0.116552\n",
       "product                          0.128541\n",
       "has_any_affiliate               -0.149690\n",
       "hasAdinText                     -0.164558\n",
       "has_any_affiliate speed          0.173918\n",
       "korean                           0.253364\n",
       "hashtag_indicator                0.368265\n",
       "dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new = sig_lasso_coeffs.sort_values(key=abs)\n",
    "new.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c996d0-a175-43db-90ce-b9fa3650814a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
