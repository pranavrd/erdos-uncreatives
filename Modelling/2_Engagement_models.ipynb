{
 "cells": [
  {
   "cell_type": "raw",
   "id": "ca315beb-9c01-4a2d-bf8f-011323d5fd6f",
   "metadata": {},
   "source": [
    "In this notebook, we will create and compare several models. Our target feature for this notebooks is (Likes+Comments)/(Views+1)\n",
    "which we will call \"engagement\". We have added 1 to the denonimator just to ensure we never divide by zero. \n",
    "\n",
    "We consider the following models:\n",
    "1) A baseline model that always outputs the average value of the target variable\n",
    "2) A basic linear regression model\n",
    "3) A linear regression model fit with lasso regression\n",
    "4) A model that includes all pairwise interaction terms\n",
    "5) A model that includeds all pairwise interaction terms fit with lasso regression.\n",
    "\n",
    "Almost as expected, the model with the most features (i.e., the model that includes all interaction terms) performed the best. This is this could just be because every time a linear model has more features it will perform better, in the sense that it will have a lower rmse. Interestingly, our lasso model slightly outperformed our non-lasso model. \n",
    "   \n",
    "Please note that we are not really creating a \"model\" of our data. That is, we do not believe our targets are fully explained by our features or any features that can be extracted from our data set. This makes sense - the views and engagement of a youtube short is primarily explained by the video content, which is not part of our data set, though things like the title and description have some effect. This explains the poor fit. However, we hope to understand something about the data from looking at these models. For example, we found that our best model had an $R^2$ score of 0.0685 and an Explained Variance of 0.0729. These numbers tell us to what extent our targets are explainable with our features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "98f96210-9b6b-4589-ae6a-be7b219121e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('../data/new/no_early_dates_all_features_train.csv')\n",
    "features = [\"popular_brand\", \"has_any_affiliate\", \"product\", \"budget\", \"self_ref\", \"korean\", \"speed\", \n",
    "            \"skills/teach\", \"comparing_products\", \"prime_hour\", \"hashtag_indicator\", \"hasAdinTitle\", \"hasAdinText\"]\n",
    "\n",
    "#Create the target column $y$ here \n",
    "\n",
    "df[\"y\"] = (df[\"likes\"] + df[\"commentsCount\"])  / (df[\"viewCount\"] + 1) \n",
    "\n",
    "#get rid of noisy columns\n",
    "df = df[ features + [\"y\"] ] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "42cf1fcf-3422-4167-9ed1-d695d3e8c59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import everything\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import root_mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bea5e3ea-744f-4ea3-a082-27bb47cd1ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do an 80-20 Train Test Split Here. Never ever touch the testing set please!\n",
    "\n",
    "df_train, df_test = train_test_split(df, shuffle = True, test_size = .2, random_state = 42) #We can't stratify because we have too many categorical features. I hope this is ok\n",
    "#DO NOT TOUCH THE ABOVE X_TEST VARIABLE FOR ANY REASON\n",
    "\n",
    "#We want a very basic idea of the MSE and other metrics for each model, before we do proper cross-validation. We use a secondary split for this.\n",
    "df_tt, df_ho = train_test_split(df_train, shuffle = True, test_size = .2, random_state = 42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "13d86438-aae7-4433-8644-e26d56daae6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error: 0.029123\n",
      "R-squared: -0.0056\n"
     ]
    }
   ],
   "source": [
    "#Create the baseline model here\n",
    "\n",
    "class BaseMeanModel():\n",
    "    def __init__(self):\n",
    "        self.mean_value = None\n",
    "    \n",
    "    def fit(self, values : pd.Series):\n",
    "        self.mean_value = values.mean()\n",
    "\n",
    "    def predict(self, inputs=None):\n",
    "        if inputs is None:\n",
    "            return self.mean_value\n",
    "        return len(inputs) * [self.mean_value]\n",
    "    \n",
    "    \n",
    "model = BaseMeanModel()\n",
    "model.fit(df_tt[\"y\"])\n",
    "\n",
    "# R2 is negative because training set and the hold out set have different average values\n",
    "y_pred = model.predict(df_ho[features])\n",
    "rmse = root_mean_squared_error(df_ho[\"y\"], y_pred)\n",
    "r2 = r2_score(df_ho[\"y\"], y_pred)\n",
    "print(f\"Root Mean Squared Error: {rmse:.6f}\")\n",
    "print(f\"R-squared: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fc720b8d-d4a5-41ed-8e0c-cce7dfb81885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error: 0.028322\n",
      "R-squared: 0.0489\n"
     ]
    }
   ],
   "source": [
    "#Creat the basic linear regression model here \n",
    "###Fitting the basic linear regression model using the training set and print the summary of the model.\n",
    "model = LinearRegression()\n",
    "model.fit(df_tt[features], df_tt[\"y\"])\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(df_ho[features])\n",
    "rmse = root_mean_squared_error(df_ho[\"y\"], y_pred)\n",
    "r2 = r2_score(df_ho[\"y\"], y_pred)\n",
    "print(f\"Root Mean Squared Error: {rmse:.6f}\")\n",
    "print(f\"R-squared: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3e891559-d14a-47a2-af18-a5363af7e60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Alpha: 0.0001\n",
      "Test MSE: 0.000804, RMSE: 0.028349, MAE: 0.021549\n",
      "R² Score: 0.047118, Explained Variance: 0.051648\n"
     ]
    }
   ],
   "source": [
    "#Create the basic linear regression model here with lasso regression. \n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LassoCV, Lasso\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, explained_variance_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(df_tt[features] )\n",
    "X_test_scaled = scaler.transform( df_ho[features])\n",
    "\n",
    "alpha = 0.0001\n",
    "lasso = Lasso(alpha=alpha, random_state=42, max_iter=10000)\n",
    "\n",
    "#cv_scores = cross_val_score(lasso, X_train_scaled, df_tt[\"y\"], cv=5,scoring='neg_mean_squared_error')\n",
    "#cv_rmse = np.sqrt(-cv_scores.mean())\n",
    "\n",
    "lasso.fit(X_train_scaled, df_tt[\"y\"])\n",
    "\n",
    "y_pred = lasso.predict(X_test_scaled)\n",
    "\n",
    "# Calculate metrics\n",
    "mse = mean_squared_error(df_ho[\"y\"], y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(df_ho[\"y\"], y_pred)\n",
    "r2 = r2_score(df_ho[\"y\"], y_pred)\n",
    "exp_var = explained_variance_score(df_ho[\"y\"], y_pred)\n",
    "\n",
    "\n",
    "print(f\"\\nAlpha: {alpha}\")\n",
    "print(f\"Test MSE: {mse:.6f}, RMSE: {rmse:.6f}, MAE: {mae:.6f}\")\n",
    "print(f\"R² Score: {r2:.6f}, Explained Variance: {exp_var:.6f}\")\n",
    "#print(f\"Cross-validated RMSE: {cv_rmse:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "39edb992-3082-4415-a155-dbd36c360774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 0.000786, RMSE: 0.028042, MAE: 0.021181\n",
      "R² Score: 0.067676, Explained Variance: 0.072002\n"
     ]
    }
   ],
   "source": [
    "#Create a model whose features include all interaction terms\n",
    "pipe = Pipeline([  (\"interaction terms\", PolynomialFeatures(degree = 2, interaction_only = True, include_bias = False) ),\n",
    "                   (\"linear model\", LinearRegression())\n",
    "]) \n",
    "#setting degree = 2 creates all pairwise interaction terms. \n",
    "\n",
    "pipe.fit( df_tt[features], df_tt[\"y\"]) \n",
    "pred = pipe.predict( df_ho[features] )\n",
    "\n",
    "mse = mean_squared_error(df_ho[\"y\"], pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(df_ho[\"y\"], pred)\n",
    "r2 = r2_score(df_ho[\"y\"], pred)\n",
    "exp_var = explained_variance_score(df_ho[\"y\"], pred)\n",
    "\n",
    "\n",
    "print(f\"Test MSE: {mse:.6f}, RMSE: {rmse:.6f}, MAE: {mae:.6f}\")\n",
    "print(f\"R² Score: {r2:.6f}, Explained Variance: {exp_var:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b43452cc-12d0-4ac3-882b-d7942693d0e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 0.000786, RMSE: 0.028029, MAE: 0.021180\n",
      "R² Score: 0.068512, Explained Variance: 0.072888\n"
     ]
    }
   ],
   "source": [
    "#Create a model with all interaction terms and lasso regression \n",
    "\n",
    "#Create a model with all interaction terms and lasso regression \n",
    "from sklearn.linear_model import Lasso, LassoCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaled_df_tt = scaler.fit_transform( df_tt[features] )\n",
    "scaled_df_ho = scaler.transform( df_ho[features] )\n",
    "\n",
    "# using lasso cv to find the best alpha\n",
    "# lasso = LassoCV(cv=5, random_state=42, max_iter=10000, alphas=np.logspace(-4, 1, 30))\n",
    "# lasso.fit(df_tt[features], df_tt[\"y\"])\n",
    "# pred = lasso.predict(df_ho[features])\n",
    "# print(\"Lasso CV MSE:\", root_mean_squared_error(df_ho[\"y\"], pred))\n",
    "# print(\"Optimal alpha:\", lasso.alpha_)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"interaction terms\", PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)),\n",
    "    (\"lasso\", Lasso(alpha=0.0001, max_iter=10000))\n",
    "])\n",
    "pipe.fit(scaled_df_tt, df_tt[\"y\"])\n",
    "pred = pipe.predict(scaled_df_ho)\n",
    "\n",
    "mse = mean_squared_error(df_ho[\"y\"], pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(df_ho[\"y\"], pred)\n",
    "r2 = r2_score(df_ho[\"y\"], pred)\n",
    "exp_var = explained_variance_score(df_ho[\"y\"], pred)\n",
    "\n",
    "\n",
    "print(f\"Test MSE: {mse:.6f}, RMSE: {rmse:.6f}, MAE: {mae:.6f}\")\n",
    "print(f\"R² Score: {r2:.6f}, Explained Variance: {exp_var:.6f}\")\n",
    "\n",
    "# Get lasso coefficients\n",
    "lasso_coeffs = pd.Series(pipe.named_steps['lasso'].coef_, index=pipe.named_steps['interaction terms'].get_feature_names_out(features))\n",
    "lasso_coeffs = lasso_coeffs[lasso_coeffs != 0]\n",
    "# print(lasso_coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2e6b6f89-6217-41e4-a47e-b0f06f1ec11b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.02912292 0.03097684 0.03660978 0.03208654 0.03187867]\n",
      " [0.02832235 0.0306132  0.03619643 0.03160241 0.0313802 ]\n",
      " [0.02834912 0.03059918 0.03619306 0.03161193 0.03137958]\n",
      " [0.02804164 0.02996715 0.0358492  0.03154629 0.03096252]\n",
      " [0.02802907 0.0299516  0.03582637 0.03144282 0.03088253]]\n"
     ]
    }
   ],
   "source": [
    "#Do cross-validation to compare all models \n",
    "\n",
    "#Model 0: Baseline Average\n",
    "#Model 1: Basic Linear Regression Model\n",
    "#Model 2: Linear Regression with Lasso\n",
    "#Model 3: Linear Regression with interaction terms\n",
    "#Model 4: Linear Regression with interactions and lasso\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "num_splits = 5\n",
    "num_models = 5\n",
    "\n",
    "kfold = KFold(num_splits,\n",
    "              random_state = 42,\n",
    "              shuffle=True)\n",
    "\n",
    "rmses = np.zeros((num_models, num_splits))\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(kfold.split(df_train)): \n",
    "\n",
    "    df_tt = df_train.iloc[train_index]\n",
    "    df_ho = df_train.iloc[test_index] \n",
    "\n",
    "    #Model 0: Baseline Average\n",
    "    model = BaseMeanModel()\n",
    "    model.fit(df_tt[\"y\"])\n",
    "    y_pred = model.predict(df_ho[features])\n",
    "    rmses[0,i] = root_mean_squared_error(df_ho[\"y\"], y_pred)\n",
    "\n",
    "    #Model 1: Basic Linear Regression Model\n",
    "    model = LinearRegression()\n",
    "    model.fit(df_tt[features], df_tt[\"y\"])\n",
    "    y_pred = model.predict(df_ho[features])\n",
    "    rmses[1,i] = root_mean_squared_error(df_ho[\"y\"], y_pred)\n",
    "\n",
    "    #Model 2: Linear Regression with Lasso\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(df_tt[features] )\n",
    "    X_test_scaled = scaler.transform( df_ho[features])\n",
    "    alpha = 0.0001\n",
    "    lasso = Lasso(alpha=alpha, random_state=42, max_iter=10000)\n",
    "    lasso.fit(X_train_scaled, df_tt[\"y\"])\n",
    "    y_pred = lasso.predict(X_test_scaled)\n",
    "    rmses[2,i] = root_mean_squared_error(df_ho[\"y\"], y_pred)\n",
    "\n",
    "    #Model 3: Linear Regression with interaction terms\n",
    "    pipe = Pipeline([  (\"interaction terms\", PolynomialFeatures(degree = 2, interaction_only = True, include_bias = False) ),\n",
    "                   (\"linear model\", LinearRegression())\n",
    "                    ]) \n",
    "    pipe.fit( df_tt[features], df_tt[\"y\"]) \n",
    "    y_pred = pipe.predict( df_ho[features] )\n",
    "    rmses[3,i] = root_mean_squared_error(df_ho[\"y\"], y_pred)\n",
    "\n",
    "    #Model 4: Linear Regression with interactions and lasso\n",
    "    scaler = StandardScaler()\n",
    "    scaled_df_tt = scaler.fit_transform( df_tt[features] )\n",
    "    scaled_df_ho = scaler.transform( df_ho[features] )\n",
    "    pipe = Pipeline([\n",
    "    (\"interaction terms\", PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)),\n",
    "    (\"lasso\", Lasso(alpha=0.0001, max_iter=10000))\n",
    "                    ])\n",
    "    pipe.fit(scaled_df_tt, df_tt[\"y\"])\n",
    "    y_pred = pipe.predict(scaled_df_ho)\n",
    "    rmses[4,i] = root_mean_squared_error(df_ho[\"y\"], y_pred)\n",
    "\n",
    "\n",
    "print(rmses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d17df561-474b-41dc-a4be-3659ec40bc0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.03213495, 0.03162292, 0.03162658, 0.03127336, 0.03122648])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmses.mean(axis = 1)\n",
    "#Pretty much as expected, models with the most features (i.e., models that included interaction terms) did the best.\n",
    "#Interestingly, Lasso regression slightly outperformed regular linear regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7dd1bfe5-da0d-4074-969c-8aacea12ba04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note that in the RMSE matrix, each ROW is a different model and each COLUMN is a CV. \n",
    "#In the above we took an average of each ROW. \n",
    "#As a sanity check we look at the full RMSE matrix and see that yes, in each column the first row is the largest number,\n",
    "#the second row is (often) the second largest number, etc. This means there is a consistent pattern as to the performance of our models\n",
    "#and it is not just random noise. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98cb8ba-44b5-4a98-99ac-6df2b4c153d3",
   "metadata": {},
   "source": [
    "---Final Interpretation---\n",
    "\n",
    "Almost as expected, the model with the most features (i.e., the model that includes all interaction terms) performed the best. This could just be because every time a linear model has more features it will perform better, in the sense that it will have a lower rmse. \n",
    "\n",
    "Interestingly, our lasso model slightly outperformed our non-lasso model. \n",
    "\n",
    "We want to take the coefficients of our highest performing model and compare them. This is where our analysis was really headed.\n",
    "Mathematically, the features with the highest coefficients are those that contribute the most to the trend line.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "abbfe974-86a9-4582-96b0-bd6c3b4f5288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "popular_brand                    -0.002824\n",
      "has_any_affiliate                 0.001706\n",
      "product                          -0.001542\n",
      "budget                           -0.000130\n",
      "self_ref                          0.002519\n",
      "                                    ...   \n",
      "comparing_products hasAdinText   -0.000429\n",
      "prime_hour hasAdinText            0.001043\n",
      "hashtag_indicator hasAdinTitle   -0.000756\n",
      "hashtag_indicator hasAdinText    -0.001165\n",
      "hasAdinTitle hasAdinText          0.000206\n",
      "Length: 73, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#A final fitting to the whole training data set \n",
    "scaler = StandardScaler()\n",
    "scaled_df = scaler.fit_transform( df[features] )\n",
    "pipe = Pipeline([\n",
    "    (\"interaction terms\", PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)),\n",
    "    (\"lasso\", Lasso(alpha=0.0001, max_iter=10000))\n",
    "                    ])\n",
    "pipe.fit(scaled_df, df[\"y\"])\n",
    "\n",
    "lasso_coeffs = pd.Series(pipe.named_steps['lasso'].coef_, index=pipe.named_steps['interaction terms'].get_feature_names_out(features))\n",
    "sig_lasso_coeffs = lasso_coeffs[lasso_coeffs != 0]\n",
    "print(sig_lasso_coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e03d7661-73cd-480a-8f74-467b86e08949",
   "metadata": {},
   "outputs": [],
   "source": [
    "new = sig_lasso_coeffs.sort_values(key=abs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "80b10feb-2632-410d-8ac8-9b31945b3b66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "popular_brand skills/teach        0.001537\n",
       "product                          -0.001542\n",
       "korean hashtag_indicator          0.001563\n",
       "has_any_affiliate                 0.001706\n",
       "skills/teach hashtag_indicator   -0.001795\n",
       "self_ref speed                   -0.001867\n",
       "product hashtag_indicator         0.002489\n",
       "self_ref                          0.002519\n",
       "popular_brand                    -0.002824\n",
       "hashtag_indicator                -0.003496\n",
       "dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba936c8-6015-4a72-98e5-4610c5b9c43d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Erdos Institute)",
   "language": "python",
   "name": "erdos_spring_2025"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
