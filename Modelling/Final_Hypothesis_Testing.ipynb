{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9628fb7-c37f-4543-9699-9813cdd81369",
   "metadata": {},
   "source": [
    "In this notebook, we use the hypotheses formed when we ran our EDA to conduct some hypothesis tests. We are most interested in whether or not a post in a certain category have significantly higher views/subscriber or engagement (likes + comments/views) on average. \n",
    "\n",
    "From our EDA on the training data, we formed the following hypotheses:\n",
    "\n",
    "1) Posts that use at least one hashtag in either the title or descriptio have significantly higher views/subscriber\n",
    "2) Posts with an affiliate link have significantly lower views/subscriber.\n",
    "3) Posts that declare sponsorship in the description (not the title) have significantly lower views/subscriber\n",
    "4) Posts from users that are verified have significantly higher engagement\n",
    "5) Posts from users that are verified have significantly lower views/subscriber\n",
    "6) Posts that mention a popular brand have significantly higher views/subscriber\n",
    "7) Posts that used keywords from the \"korean\" keyword group have higher views/subscriber...\n",
    "8) ...as well as higher engagement\n",
    "9) Posts that were posted during a \"prime hour\" have significantly higher views/subcriber\n",
    "10) Posts that were posted during a \"prime hour\" have significantly higher engagement.\n",
    "\n",
    "We also (as a secondary part of our EDA) analyzed the coefficients on a linear model with all categorical features of the targets y1 = views/subscriber, abnd y2 = engagement. We ran t-tests (on the training data) based on the largest coefficients and found that: \n",
    "\n",
    "11) Posts that mention a popular brand have decreased engagement.\n",
    "12) Posts that contain at least one hashtag have decreased engagement.\n",
    "    \n",
    "13-15) Videos that contain keywords from the \"comparing_products\" or \"products\" keyword group decrease engagement, whereas videos with keywords from the \"self_ref\" keyword group increase engagement\n",
    "\n",
    "We also found based on our linear models that posts that used keywords from the \"korean\" keyword group have higher views/subscriber, which was already confirmed by our first EDA. \n",
    "\n",
    "We will run several t-tests to confirm if these hypotheses are true or not on the testing set. To account for the fact that we are running multiple hypothesis tests, we will lower our significance level from the standard alpha = 0.05 to alpha = 0.005. This means we will have a 7.5% chance of a type 1 error if we run 15 tests. Keep in mind that at the very beginning we also ran two t-tests with alpha = 0.005 to determine if the data from the first two months had significantly lower views. This puts our total error rate at 8.5%. \n",
    "\n",
    "Our type II error rate is not as important. It is possible, for example, that we missed the fact that including a hashtag increases views. However, this is fine for our use case. We would rather miss out on recommendations to make than make recommendations that don't work. This is an important point also for power calculation considerations. We are forced to conduct our analysis on data that is already existing and do not have the ability to design a future study with a desired sample size necessary to make sure our study has a certain desired power. \n",
    "\n",
    "Next, we should see if these statistically significant differences are practically significant. What constitutes practical significance will be subjective here. We will be using an ex-post analysis of MDE (minimum detectble effect) to find out if our differences are practically significant, as inspired by [this blog post](https://blogs.worldbank.org/en/impactevaluations/why-ex-post-power-using-estimated-effect-sizes-bad-ex-post-mde-not)  \n",
    "\n",
    "For views/subscriber, we will say that an increase of .1 is practically significant. If a user has 100,000 subscribers, this means that the video got an extra 10,000 views. If a user has 10,000 subscribers, this means that the video got an extra 1,000 views. This is a subjective judgment call.\n",
    "\n",
    "I belive this means that our desired value of Cohen's $d$ is $0.1 / \\sqrt{ \\sigma_1^2 + \\sigma_2^2 }$ or larger. \n",
    "\n",
    "For engagement, it is hard to say what is significant. We will use the following standard rough estimates: A value of 0.2 represents a small effect size. \n",
    "A value of 0.5 represents a medium effect size \r\n",
    "A value of 0.8 represents a large effect size  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f7d33ed4-e11a-4f51-b157-f8ef7fcb89b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'any_ht', 'commentsCount', 'isChannelVerified', 'likes',\n",
       "       'numberOfSubscribers', 'text', 'title', 'viewCount',\n",
       "       'views_per_subscriber', 'duration_in_seconds', 'date',\n",
       "       'hashtag_indicator', 'has_any_affiliate', 'hasAdinTitle', 'hasAdinText',\n",
       "       'Engagement_per_Subscriber', 'Engagement_per_View', 'popular_brand',\n",
       "       'prime_hour', 'product', 'skills/teach', 'speed', 'comparing_products',\n",
       "       'self_ref', 'budget', 'korean', 'engagement', 'views'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "df = pd.read_csv('../data/new/no_early_dates_all_features_test.csv')\n",
    "#Creating the missing target variables\n",
    "df[\"engagement\"] = (df[\"likes\"] + df[\"commentsCount\"])  / (df[\"viewCount\"] + 1) \n",
    "df[\"views\"] = (df[\"viewCount\"]) / ( df[\"numberOfSubscribers\"] + 1) \n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "990e515f-530f-472b-ae6e-42aea39bade6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note that we will be using Welch's t-test instead; we have gone ahead and printed the variances of all the groups involved as we run the following code\n",
    "#to show why we should be doing this. \n",
    "\n",
    "#First we need to create a categorical hashtag variable\n",
    "df[\"any_ht\"] = 1 * df[\"any_ht\"].astype(bool)\n",
    "\n",
    "views_features = [\"any_ht\", \"has_any_affiliate\", \"hasAdinText\", \"isChannelVerified\", \"popular_brand\", \"korean\", \"prime_hour\"]\n",
    "eng_features = [\"isChannelVerified\", \"korean\", \"prime_hour\", \"popular_brand\", \"any_ht\", \"comparing_products\", \"product\", \"self_ref\"] \n",
    "\n",
    "def cohen(x,y): \n",
    "    '''x is the series from the yes df, y is no'''\n",
    "    mean_with = x.mean()\n",
    "    mean_without = y.mean()\n",
    "    std_with = x.std(ddof=1)\n",
    "    std_without = y.std(ddof=1)\n",
    "    n_with = len(x)\n",
    "    n_without = len(y) \n",
    "    pooled_std = np.sqrt(((n_with - 1) * std_with**2 + (n_without - 1) * std_without**2) / (n_with + n_without - 2))\n",
    "    cohens_d = abs( (mean_with - mean_without) / pooled_std )\n",
    "    desired_d = abs(0.1 / pooled_std) \n",
    "    #we take the abs since which group we expect to be higher vs lower depends on the test \n",
    "    return (cohens_d, desired_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fdd9a30a-19c1-49dc-9c05-8fcc57df8569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "any_ht\n",
      "p-value: 2.2368813261800056e-05\n",
      "t-stat: 4.083816568105291\n",
      "Pass: True\n",
      "Cohen's d: 0.08166307936719766\n",
      "Desired d: 0.018614694438988753\n",
      "Desired difference obtained: True\n",
      "variances: 38.600522273288675 11.268364658202877\n",
      "has_any_affiliate\n",
      "p-value: 2.326208027584433e-12\n",
      "t-stat: -6.930613675538518\n",
      "Pass: True\n",
      "Cohen's d: 0.10317966863463907\n",
      "Desired d: 0.01860973050520539\n",
      "Desired difference obtained: True\n",
      "variances: 1.5667041297945037 32.05638862504538\n",
      "hasAdinText\n",
      "p-value: 0.3847060669838128\n",
      "t-stat: 0.29319192748459416\n",
      "Pass: False\n",
      "Cohen's d: 0.009001836449133846\n",
      "Desired d: 0.0186005647314472\n",
      "Desired difference obtained: False\n",
      "variances: 27.09204363059775 29.220039282372912\n",
      "isChannelVerified\n",
      "p-value: 6.965578537378104e-13\n",
      "t-stat: -7.098876241368914\n",
      "Pass: True\n",
      "Cohen's d: 0.12607301523049674\n",
      "Desired d: 0.01863365138262357\n",
      "Desired difference obtained: True\n",
      "variances: 2.7178139581688323 42.275963786372095\n",
      "popular_brand\n",
      "p-value: 0.003566895227436586\n",
      "t-stat: -2.6916694132460037\n",
      "Pass: True\n",
      "Cohen's d: 0.04721684799127134\n",
      "Desired d: 0.01860318672006871\n",
      "Desired difference obtained: True\n",
      "variances: 4.830906603154853 33.309647775579364\n",
      "korean\n",
      "p-value: 1.9683027635063296e-05\n",
      "t-stat: 4.146771396809218\n",
      "Pass: True\n",
      "Cohen's d: 0.48952664362319026\n",
      "Desired d: 0.018735484196382744\n",
      "Desired difference obtained: True\n",
      "variances: 203.7225309059297 16.326070479286987\n",
      "prime_hour\n",
      "p-value: 0.04582592760237109\n",
      "t-stat: -1.687029116929263\n",
      "Pass: False\n",
      "Cohen's d: 0.035778027873058095\n",
      "Desired d: 0.018603425908613417\n",
      "Desired difference obtained: True\n",
      "variances: 7.25480481048998 47.252575933148236\n"
     ]
    }
   ],
   "source": [
    "for feature in views_features: \n",
    "    yes = df.loc[ df[feature] == 1]\n",
    "    no = df.loc[ df[feature] == 0]\n",
    "    t_stat, p_value = stats.ttest_ind( yes[\"views\"], no[\"views\"], equal_var=False ) \n",
    "    print(feature)\n",
    "    print(\"p-value:\", p_value/2) #This is technically a one-tailed test so we divide the p-value by 2\n",
    "    print(\"t-stat:\", t_stat)\n",
    "    print(\"Pass:\", p_value/2 < 0.005)\n",
    "    print(\"Cohen's d:\", cohen( yes[\"views\"], no[\"views\"])[0]  )\n",
    "    print(\"Desired d:\", cohen( yes[\"views\"], no[\"views\"])[1]  )\n",
    "    print(\"Desired difference obtained:\", cohen( yes[\"views\"], no[\"views\"])[0] > cohen( yes[\"views\"], no[\"views\"])[1] )\n",
    "    print(\"variances:\", yes[\"views\"].var(), no[\"views\"].var())\n",
    "    #print(\"variance:\", yes[\"views\"].var, no[\"views\"].var)\n",
    "    #When t_stat>0 we know that the \"yes\" group mean is greater than the \"no\" group mean. Otherwise, it's less than. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "97a920b9-b242-4da0-918c-493a979c29d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "isChannelVerified\n",
      "p-value: 1.8424588246660634e-22\n",
      "t-stat: 9.711456550257012\n",
      "Pass: True\n",
      "Cohen's d: 0.20688476655837193\n",
      "Effect size: Small\n",
      "variances: 2.7178139581688323 42.275963786372095\n",
      "korean\n",
      "p-value: 0.06469617079865729\n",
      "t-stat: 1.5184830117894588\n",
      "Pass: False\n",
      "Cohen's d: 0.05697002981458164\n",
      "Not a significant effect size.\n",
      "variances: 203.7225309059297 16.326070479286987\n",
      "prime_hour\n",
      "p-value: 2.397052061507639e-09\n",
      "t-stat: 5.86142932445298\n",
      "Pass: True\n",
      "Cohen's d: 0.1339735017390076\n",
      "Not a significant effect size.\n",
      "variances: 7.25480481048998 47.252575933148236\n",
      "popular_brand\n",
      "p-value: 7.165796787125945e-26\n",
      "t-stat: -10.625127727251936\n",
      "Pass: True\n",
      "Cohen's d: 0.3350873622164535\n",
      "Effect size: Small\n",
      "variances: 4.830906603154853 33.309647775579364\n",
      "any_ht\n",
      "p-value: 1.137741311926081e-47\n",
      "t-stat: -14.587929735696385\n",
      "Pass: True\n",
      "Cohen's d: 0.34209686442231635\n",
      "Effect size: Small\n",
      "variances: 38.600522273288675 11.268364658202877\n",
      "comparing_products\n",
      "p-value: 2.041758511745789e-08\n",
      "t-stat: -5.527548650061268\n",
      "Pass: True\n",
      "Cohen's d: 0.18253445458060402\n",
      "Not a significant effect size.\n",
      "variances: 41.65342533494001 27.44246485246804\n",
      "product\n",
      "p-value: 6.812831933888577e-05\n",
      "t-stat: -3.820528548627721\n",
      "Pass: True\n",
      "Cohen's d: 0.10039017667328828\n",
      "Not a significant effect size.\n",
      "variances: 53.859700020517785 22.758204791646623\n",
      "self_ref\n",
      "p-value: 0.005869779219204239\n",
      "t-stat: 2.5204618832049106\n",
      "Pass: False\n",
      "Cohen's d: 0.05646010405333533\n",
      "Not a significant effect size.\n",
      "variances: 40.26693642249657 18.096428151578813\n"
     ]
    }
   ],
   "source": [
    "for feature in eng_features: \n",
    "    yes = df.loc[ df[feature] == 1]\n",
    "    no = df.loc[ df[feature] == 0]\n",
    "    t_stat, p_value = stats.ttest_ind( yes[\"engagement\"], no[\"engagement\"], equal_var=False ) \n",
    "    print(feature)\n",
    "    print(\"p-value:\", p_value/2)\n",
    "    print(\"t-stat:\", t_stat)\n",
    "    print(\"Pass:\", p_value/2 < 0.005)\n",
    "    print(\"Cohen's d:\", cohen( yes[\"engagement\"], no[\"engagement\"])[0]  )\n",
    "\n",
    "    if cohen( yes[\"engagement\"], no[\"engagement\"])[0] < 0.2:\n",
    "        print(\"Not a significant effect size.\")\n",
    "    elif 0.2 <= cohen( yes[\"engagement\"], no[\"engagement\"])[0] < 0.5:\n",
    "        print(\"Effect size: Small\")\n",
    "    elif 0.5 <= cohen( yes[\"engagement\"], no[\"engagement\"])[0] < 0.8:\n",
    "        print(\"Effect size: Medium\")\n",
    "    elif cohen( yes[\"engagement\"], no[\"engagement\"])[0] >= 0.8:\n",
    "        print(\"Effect size: Large\") \n",
    "    \n",
    "    print(\"variances:\", yes[\"views\"].var(), no[\"views\"].var())\n",
    "    #When t_stat>0 we know that the \"yes\" group mean is greater than the \"no\" group mean. Otherwise, it's less than. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291ff76e-048a-47d3-8811-85a598173973",
   "metadata": {},
   "source": [
    "# Final Results:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e011a6-c0cd-4689-82cb-ddd1d36d781a",
   "metadata": {},
   "source": [
    "We have confirmed that the following posts have significantly different views/subscriber on average:\n",
    "1) Posts that contain at least one hashtag in the title or description have higher views/subscriber with a desired effect size \n",
    "2) Posts that contain an affiliate link have lower views/subscriber with a desired effect size\n",
    "3) Posts from users that are verified have lower views/subscriber with a desired effect size \n",
    "4) Posts that mention a popular brand have higher views/subscriber with a desired effect size \n",
    "5) Posts that contain a keyword from the \"korean\" keyword group have higher views/subscriber with a desired effect size \n",
    "\n",
    "We have confirmed that the following posts have significantly different engagement rates on average:\n",
    "1) Posts from users that are verified have higher engagement with a small effect size \n",
    "2) Posts posted during a prime hour have higher engagement but without a significant effect size \n",
    "3) Posts that mention a popular brand have lower engagement with a small effect size \n",
    "4) Posts that contain at least one hashtag in the title or description have lower engagement with a small effect size \n",
    "5) Posts that contain at least one keyword from the \"comparing_products\" keyword group have lower engagement but without a significant effect size \n",
    "6) Posts that contain at least one keyword from the \"product\" keyword group have lower engagement but without a significant effect size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226bd037-c59b-4aae-9626-286a5c6881f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60799938-ccf7-4ae9-8798-6e77de9eb828",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
