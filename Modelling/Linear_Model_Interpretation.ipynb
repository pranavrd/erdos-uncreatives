{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "id": "5vX0c4JOQYMp",
    "outputId": "8de64bd2-20af-4062-9b0f-b4dec05b1eae"
   },
   "source": [
    "In this notebook which look at which features had the largest impact in our linear model. Here, we re-define our features slightly to be all categorical. We then run t-tests (on the training data only) to see if those features have significantly higher views or significantly higher engagement. \n",
    "\n",
    "As a final conclusion: Videos that contain keywords from the \"korean\" keyword group seem to have significantly more views.\n",
    "Videos that mention a popular brand or contain at least one hashtag decrease engagement. \n",
    "Videos that contain keywords from the \"comparing_products\" or \"products\" keyword group decrease engagement, whereas videos with keywords from the \"self_ref\" keyword group increase engagement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "rQlhGaw8QgYn"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../data/new/no_early_dates_all_features_train.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1FAzACGrQvwy",
    "outputId": "bc5ea4fd-e147-47eb-b65b-c045bff81074"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'commentsCount', 'isChannelVerified', 'likes',\n",
       "       'numberOfSubscribers', 'text', 'title', 'viewCount',\n",
       "       'views_per_subscriber', 'duration_in_seconds', 'date',\n",
       "       'hashtag_indicator', 'has_any_affiliate', 'hasAdinTitle', 'hasAdinText',\n",
       "       'Engagement_per_Subscriber', 'Engagement_per_View', 'popular_brand',\n",
       "       'prime_hour', 'product', 'skills/teach', 'speed', 'comparing_products',\n",
       "       'self_ref', 'budget', 'korean'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "smyHNMO3VAVn"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#Changing hashtags to a categorical variable \n",
    "df[\"hashtag_indicator\"] = 1 * df[\"hashtag_indicator\"].astype(bool)\n",
    "\n",
    "features = [\"popular_brand\", \"has_any_affiliate\", \"product\", \"budget\", \"self_ref\", \"korean\", \"speed\", \"skills/teach\", \"comparing_products\", \"prime_hour\", \"hasAdinTitle\", \"hasAdinText\",'hashtag_indicator']\n",
    "\n",
    "#Create the target column $y$ here\n",
    "df[\"y\"] = (df[\"likes\"] + df[\"commentsCount\"])  / (df[\"viewCount\"] + 1)\n",
    "df[\"y2\"] = (df[\"viewCount\"]) / ( df[\"numberOfSubscribers\"] + 1) \n",
    "\n",
    "#get rid of noisy columns\n",
    "df = df[ features + [\"y\"] + [\"y2\"] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JorvUfeNQv53",
    "outputId": "46ab5c78-c5a2-4352-e1b5-8a47a7fcdc0f"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into exploration set and confirmation\n",
    "#df_explore, df_confirm = train_test_split(df, test_size=0.5, random_state=42)\n",
    "\n",
    "#print(f\"Exploration set: {df_explore.shape[0]} rows\")\n",
    "#print(f\"Confirmation set: {df_confirm.shape[0]} rows\")\n",
    "\n",
    "#Don't actually need to do a train-test split since we imported the training data to begin with\n",
    "df_explore = df #this allows the rest of the code to continue to run \n",
    "df_confirm = df #All tests and exploration are going to take place on the training data we imported; final tests will be based on EDA as well as linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NWWP9t0WQwAI",
    "outputId": "10e186c3-f06d-442f-c91d-9c7ed2c8c614"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_explore_scaled = scaler.fit_transform(df_explore[features])\n",
    "\n",
    "pipe_linear = Pipeline([\n",
    "    (\"interaction_terms\", PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)),\n",
    "    (\"lasso\", Lasso(alpha=0.0001, max_iter=10000))\n",
    "])\n",
    "pipe_linear.fit(X_explore_scaled, df_explore[\"y\"])\n",
    "\n",
    "linear_pred = pipe_linear.predict(X_explore_scaled)\n",
    "linear_mse = mean_squared_error(df_explore[\"y\"], linear_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XM09X92AQwDI",
    "outputId": "23eb76bd-6bcf-4b72-a388-fc71dbae423d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All non-zero features sorted by coefficient magnitude:\n",
      "speed skills/teach                      0.000011\n",
      "comparing_products hashtag_indicator    0.000022\n",
      "product speed                           0.000029\n",
      "budget skills/teach                    -0.000030\n",
      "korean hasAdinTitle                    -0.000042\n",
      "                                          ...   \n",
      "korean speed                            0.001501\n",
      "has_any_affiliate                       0.002245\n",
      "popular_brand                          -0.002897\n",
      "self_ref                                0.002901\n",
      "hashtag_indicator                      -0.004031\n",
      "Length: 74, dtype: float64\n",
      "\n",
      "Top 10 most important features:\n",
      "comparing_products               -0.001185\n",
      "product                          -0.001287\n",
      "skills/teach hashtag_indicator   -0.001350\n",
      "popular_brand skills/teach        0.001364\n",
      "product hashtag_indicator         0.001378\n",
      "korean speed                      0.001501\n",
      "has_any_affiliate                 0.002245\n",
      "popular_brand                    -0.002897\n",
      "self_ref                          0.002901\n",
      "hashtag_indicator                -0.004031\n",
      "dtype: float64\n",
      "\n",
      "Formulated hypotheses:\n",
      "1. Videos with 'comparing_products' decrease engagement (coef=-0.001185)\n",
      "2. Videos with 'product' decrease engagement (coef=-0.001287)\n",
      "3. Videos that combine 'skills/teach' and 'hashtag_indicator' decrease engagement (coef=-0.001350)\n",
      "4. Videos that combine 'popular_brand' and 'skills/teach' increase engagement (coef=0.001364)\n",
      "5. Videos that combine 'product' and 'hashtag_indicator' increase engagement (coef=0.001378)\n",
      "6. Videos that combine 'korean' and 'speed' increase engagement (coef=0.001501)\n",
      "7. Videos with 'has_any_affiliate' increase engagement (coef=0.002245)\n",
      "8. Videos with 'popular_brand' decrease engagement (coef=-0.002897)\n",
      "9. Videos with 'self_ref' increase engagement (coef=0.002901)\n",
      "10. Videos with 'hashtag_indicator' decrease engagement (coef=-0.004031)\n"
     ]
    }
   ],
   "source": [
    "# Extract coefficients\n",
    "transformed_feature_names = pipe_linear.named_steps['interaction_terms'].get_feature_names_out(features)\n",
    "lasso_coeffs = pd.Series(\n",
    "    pipe_linear.named_steps['lasso'].coef_,\n",
    "    index=transformed_feature_names\n",
    ")\n",
    "\n",
    "# Get non-zero coefficients\n",
    "sig_lasso_coeffs = lasso_coeffs[lasso_coeffs != 0]\n",
    "important_features = sig_lasso_coeffs.sort_values(key=abs)\n",
    "\n",
    "print(\"\\nAll non-zero features sorted by coefficient magnitude:\")\n",
    "print(important_features)\n",
    "\n",
    "print(\"\\nTop 10 most important features:\")\n",
    "top_features = important_features.tail(10)\n",
    "print(top_features)\n",
    "\n",
    "# create hypotheses\n",
    "hypotheses = []\n",
    "for feature, coef in top_features.items():\n",
    "    expected_direction = \"positive\" if coef > 0 else \"negative\"\n",
    "    impact = \"increase\" if coef > 0 else \"decrease\"\n",
    "\n",
    "    if \" \" in feature:\n",
    "        # For interaction terms\n",
    "        parts = feature.split(\" \")\n",
    "        hypothesis_text = f\"Videos that combine '{parts[0]}' and '{parts[1]}' {impact} engagement\"\n",
    "    else:\n",
    "        # For single features\n",
    "        hypothesis_text = f\"Videos with '{feature}' {impact} engagement\"\n",
    "\n",
    "    hypotheses.append({\n",
    "        \"feature\": feature,\n",
    "        \"coefficient\": coef,\n",
    "        \"expected_direction\": expected_direction,\n",
    "        \"hypothesis\": hypothesis_text\n",
    "    })\n",
    "\n",
    "print(\"\\nFormulated hypotheses:\")\n",
    "for i, hyp in enumerate(hypotheses, 1):\n",
    "    print(f\"{i}. {hyp['hypothesis']} (coef={hyp['coefficient']:.6f})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jBDxCjUKHRXt",
    "outputId": "dffa0f7b-e783-4d3f-faf3-f8c997b78479"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['popular_brand', 'has_any_affiliate', 'product', 'budget', 'self_ref',\n",
       "       'korean', 'speed', 'skills/teach', 'comparing_products', 'prime_hour',\n",
       "       'hasAdinTitle', 'hasAdinText', 'hashtag_indicator', 'y', 'y2'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_confirm.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iX4jl0T7C7z5",
    "outputId": "fd36ad7b-68aa-483a-b9c7-b3cef2c703b1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'feature': 'comparing_products',\n",
       "  'coefficient': -0.0011854043620507509,\n",
       "  'expected_direction': 'negative',\n",
       "  'hypothesis': \"Videos with 'comparing_products' decrease engagement\"},\n",
       " {'feature': 'product',\n",
       "  'coefficient': -0.0012871632821651372,\n",
       "  'expected_direction': 'negative',\n",
       "  'hypothesis': \"Videos with 'product' decrease engagement\"},\n",
       " {'feature': 'skills/teach hashtag_indicator',\n",
       "  'coefficient': -0.0013499872960675242,\n",
       "  'expected_direction': 'negative',\n",
       "  'hypothesis': \"Videos that combine 'skills/teach' and 'hashtag_indicator' decrease engagement\"},\n",
       " {'feature': 'popular_brand skills/teach',\n",
       "  'coefficient': 0.0013637794553379501,\n",
       "  'expected_direction': 'positive',\n",
       "  'hypothesis': \"Videos that combine 'popular_brand' and 'skills/teach' increase engagement\"},\n",
       " {'feature': 'product hashtag_indicator',\n",
       "  'coefficient': 0.0013776152077387904,\n",
       "  'expected_direction': 'positive',\n",
       "  'hypothesis': \"Videos that combine 'product' and 'hashtag_indicator' increase engagement\"},\n",
       " {'feature': 'korean speed',\n",
       "  'coefficient': 0.0015013530156033092,\n",
       "  'expected_direction': 'positive',\n",
       "  'hypothesis': \"Videos that combine 'korean' and 'speed' increase engagement\"},\n",
       " {'feature': 'has_any_affiliate',\n",
       "  'coefficient': 0.0022447432201303484,\n",
       "  'expected_direction': 'positive',\n",
       "  'hypothesis': \"Videos with 'has_any_affiliate' increase engagement\"},\n",
       " {'feature': 'popular_brand',\n",
       "  'coefficient': -0.002897388840584754,\n",
       "  'expected_direction': 'negative',\n",
       "  'hypothesis': \"Videos with 'popular_brand' decrease engagement\"},\n",
       " {'feature': 'self_ref',\n",
       "  'coefficient': 0.002900942774922632,\n",
       "  'expected_direction': 'positive',\n",
       "  'hypothesis': \"Videos with 'self_ref' increase engagement\"},\n",
       " {'feature': 'hashtag_indicator',\n",
       "  'coefficient': -0.0040313595429592244,\n",
       "  'expected_direction': 'negative',\n",
       "  'hypothesis': \"Videos with 'hashtag_indicator' decrease engagement\"}]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "-X_hRFq_d4sb"
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Test each hypothesis on the confirmation dataset\n",
    "results = []\n",
    "\n",
    "for hypothesis in hypotheses:\n",
    "    feature = hypothesis[\"feature\"]\n",
    "    expected_direction = hypothesis[\"expected_direction\"]\n",
    "\n",
    "    if \" \" not in feature:\n",
    "        if feature not in df_confirm.columns:\n",
    "            results.append({\n",
    "                'feature': feature,\n",
    "                'hypothesis': hypothesis[\"hypothesis\"],\n",
    "                'result': \"Feature not found in dataset\",\n",
    "                'confirmed': False\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        # Group data based on feature presence\n",
    "        with_feature = df_confirm[df_confirm[feature] == 1][\"y\"]\n",
    "        without_feature = df_confirm[df_confirm[feature] == 0][\"y\"]\n",
    "\n",
    "        # Skip if either group is too small\n",
    "        if len(with_feature) < 10 or len(without_feature) < 10:\n",
    "            results.append({\n",
    "                'feature': feature,\n",
    "                'hypothesis': hypothesis[\"hypothesis\"],\n",
    "                'result': f\"Insufficient data (with={len(with_feature)}, without={len(without_feature)})\",\n",
    "                'confirmed': False\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        #  t-test\n",
    "        t_stat, p_value = stats.ttest_ind(with_feature, without_feature, equal_var=False)\n",
    "\n",
    "        # Check if result confirms hypothesis direction\n",
    "        mean_diff = with_feature.mean() - without_feature.mean()\n",
    "        direction_confirmed = (\n",
    "            (expected_direction == \"positive\" and mean_diff > 0) or\n",
    "            (expected_direction == \"negative\" and mean_diff < 0)\n",
    "        )\n",
    "\n",
    "        # Record results\n",
    "        results.append({\n",
    "            'feature': feature,\n",
    "            'hypothesis': hypothesis[\"hypothesis\"],\n",
    "            'mean_with': with_feature.mean(),\n",
    "            'mean_without': without_feature.mean(),\n",
    "            'difference': mean_diff,\n",
    "            'p_value': p_value,\n",
    "            'significant': p_value < 0.05,\n",
    "            't_statistic': t_stat,\n",
    "            'expected_direction': expected_direction,\n",
    "            'actual_direction': \"positive\" if mean_diff > 0 else \"negative\",\n",
    "            'direction_confirmed': direction_confirmed,\n",
    "            'hypothesis_confirmed': direction_confirmed and p_value < 0.05\n",
    "        })\n",
    "\n",
    "    # For interaction terms (simplified approach)\n",
    "    else:\n",
    "        feature_parts = feature.split(\" \")\n",
    "        if len(feature_parts) != 2:\n",
    "            results.append({\n",
    "                'feature': feature,\n",
    "                'hypothesis': hypothesis[\"hypothesis\"],\n",
    "                'result': \"Complex interaction term - not tested\",\n",
    "                'confirmed': False\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        feature1, feature2 = feature_parts\n",
    "\n",
    "        # Check if features exist\n",
    "        if feature1 not in df_confirm.columns or feature2 not in df_confirm.columns:\n",
    "            results.append({\n",
    "                'feature': feature,\n",
    "                'hypothesis': hypothesis[\"hypothesis\"],\n",
    "                'result': \"One or more features not found in dataset\",\n",
    "                'confirmed': False\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        # Create groups for interaction\n",
    "        both_present = df_confirm[(df_confirm[feature1] == 1) & (df_confirm[feature2] == 1)][\"y\"]\n",
    "        not_both = df_confirm[~((df_confirm[feature1] == 1) & (df_confirm[feature2] == 1))][\"y\"]\n",
    "\n",
    "        # Skip if either group is too small\n",
    "        if len(both_present) < 10 or len(not_both) < 10:\n",
    "            results.append({\n",
    "                'feature': feature,\n",
    "                'hypothesis': hypothesis[\"hypothesis\"],\n",
    "                'result': f\"Insufficient data (both={len(both_present)}, not_both={len(not_both)})\",\n",
    "                'confirmed': False\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        # Perform t-test\n",
    "        t_stat, p_value = stats.ttest_ind(both_present, not_both, equal_var=False)\n",
    "\n",
    "        # Check direction\n",
    "        mean_diff = both_present.mean() - not_both.mean()\n",
    "        direction_confirmed = (\n",
    "            (expected_direction == \"positive\" and mean_diff > 0) or\n",
    "            (expected_direction == \"negative\" and mean_diff < 0)\n",
    "        )\n",
    "\n",
    "        # Record results\n",
    "        results.append({\n",
    "            'feature': feature,\n",
    "            'hypothesis': hypothesis[\"hypothesis\"],\n",
    "            'mean_both': both_present.mean(),\n",
    "            'mean_not_both': not_both.mean(),\n",
    "            'difference': mean_diff,\n",
    "            'p_value': p_value,\n",
    "            'significant': p_value < 0.05,\n",
    "            't_statistic': t_stat,\n",
    "            'expected_direction': expected_direction,\n",
    "            'actual_direction': \"positive\" if mean_diff > 0 else \"negative\",\n",
    "            'direction_confirmed': direction_confirmed,\n",
    "            'hypothesis_confirmed': direction_confirmed and p_value < 0.05\n",
    "        })\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X7GQQCOkMtiM",
    "outputId": "3ba9dc6f-8740-4b66-ee69-d25d8e303b7f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 14)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hWODFw-8d4wM",
    "outputId": "8929cef4-3be2-4b87-f348-ba454ae786aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results Summary: 7 out of 10 hypotheses confirmed (70.0%)\n",
      "\n",
      "Confirmed Hypotheses:\n",
      "- Videos with 'comparing_products' decrease engagement (p=0.0001)\n",
      "  Mean with comparing_products: 0.0524\n",
      "  Mean without comparing_products: 0.0569\n",
      "  Difference: -0.0044\n",
      "- Videos with 'product' decrease engagement (p=0.0076)\n",
      "  Mean with product: 0.0546\n",
      "  Mean without product: 0.0569\n",
      "  Difference: -0.0024\n",
      "- Videos that combine 'skills/teach' and 'hashtag_indicator' decrease engagement (p=0.0001)\n",
      "  Mean with skills/teach hashtag_indicator: nan\n",
      "  Mean without skills/teach hashtag_indicator: nan\n",
      "  Difference: -0.0045\n",
      "- Videos that combine 'korean' and 'speed' increase engagement (p=0.0000)\n",
      "  Mean with korean speed: nan\n",
      "  Mean without korean speed: nan\n",
      "  Difference: 0.0114\n",
      "- Videos with 'popular_brand' decrease engagement (p=0.0000)\n",
      "  Mean with popular_brand: 0.0480\n",
      "  Mean without popular_brand: 0.0580\n",
      "  Difference: -0.0100\n",
      "- Videos with 'self_ref' increase engagement (p=0.0004)\n",
      "  Mean with self_ref: 0.0578\n",
      "  Mean without self_ref: 0.0552\n",
      "  Difference: 0.0027\n",
      "- Videos with 'hashtag_indicator' decrease engagement (p=0.0000)\n",
      "  Mean with hashtag_indicator: 0.0543\n",
      "  Mean without hashtag_indicator: 0.0588\n",
      "  Difference: -0.0044\n",
      "\n",
      "Unconfirmed Hypotheses:\n",
      "- Videos that combine 'popular_brand' and 'skills/teach' increase engagement (p=0.4296)\n",
      "- Videos that combine 'product' and 'hashtag_indicator' increase engagement (p=0.6652)\n",
      "- Videos with 'has_any_affiliate' increase engagement (p=0.1241)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "confirmed_hypotheses = results_df[results_df['hypothesis_confirmed'] == True]\n",
    "confirmation_rate = len(confirmed_hypotheses) / len(results_df) * 100\n",
    "\n",
    "print(f\"\\nResults Summary: {len(confirmed_hypotheses)} out of {len(results_df)} hypotheses confirmed ({confirmation_rate:.1f}%)\")\n",
    "\n",
    "# confirmed hypotheses\n",
    "print(\"\\nConfirmed Hypotheses:\")\n",
    "for i, row in confirmed_hypotheses.iterrows():\n",
    "    if 'mean_with' in row:\n",
    "        print(f\"- {row['hypothesis']} (p={row['p_value']:.4f})\")\n",
    "        print(f\"  Mean with {row['feature']}: {row['mean_with']:.4f}\")\n",
    "        print(f\"  Mean without {row['feature']}: {row['mean_without']:.4f}\")\n",
    "        print(f\"  Difference: {row['difference']:.4f}\")\n",
    "    else:\n",
    "        print(f\"- {row['hypothesis']} (p={row['p_value']:.4f})\")\n",
    "        print(f\"  Mean with both features: {row['mean_both']:.4f}\")\n",
    "        print(f\"  Mean without both features: {row['mean_not_both']:.4f}\")\n",
    "        print(f\"  Difference: {row['difference']:.4f}\")\n",
    "# unconfirmed hypotheses\n",
    "unconfirmed = results_df[results_df['hypothesis_confirmed'] != True]\n",
    "print(\"\\nUnconfirmed Hypotheses:\")\n",
    "\n",
    "for i, row in unconfirmed.iterrows():\n",
    "    if 'p_value' in row and row['p_value'] is not None:\n",
    "        print(f\"- {row['hypothesis']} (p={row['p_value']:.4f})\")\n",
    "    else:\n",
    "        print(f\"- {row['hypothesis']} (Reason: {row['result']})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v7Am1VHBd40L"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we re-run all the above code with regard to y2 = views/subscriber instead\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_explore_scaled = scaler.fit_transform(df_explore[features])\n",
    "\n",
    "pipe_linear = Pipeline([\n",
    "    (\"interaction_terms\", PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)),\n",
    "    (\"lasso\", Lasso(alpha=0.0001, max_iter=10000))\n",
    "])\n",
    "pipe_linear.fit(X_explore_scaled, df_explore[\"y2\"])\n",
    "\n",
    "linear_pred = pipe_linear.predict(X_explore_scaled)\n",
    "linear_mse = mean_squared_error(df_explore[\"y2\"], linear_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All non-zero features sorted by coefficient magnitude:\n",
      "self_ref hasAdinTitle            -0.001582\n",
      "has_any_affiliate skills/teach    0.006736\n",
      "korean speed                      0.007015\n",
      "skills/teach hasAdinTitle         0.009862\n",
      "self_ref skills/teach             0.012974\n",
      "                                    ...   \n",
      "popular_brand product             0.990829\n",
      "product speed                     1.126216\n",
      "speed prime_hour                  1.426781\n",
      "popular_brand prime_hour          1.534968\n",
      "popular_brand speed               1.718733\n",
      "Length: 91, dtype: float64\n",
      "\n",
      "Top 10 most important features:\n",
      "speed hasAdinText          -0.637584\n",
      "korean                      0.718873\n",
      "product prime_hour          0.813412\n",
      "popular_brand               0.844806\n",
      "prime_hour hasAdinText     -0.891425\n",
      "popular_brand product       0.990829\n",
      "product speed               1.126216\n",
      "speed prime_hour            1.426781\n",
      "popular_brand prime_hour    1.534968\n",
      "popular_brand speed         1.718733\n",
      "dtype: float64\n",
      "\n",
      "Formulated hypotheses:\n",
      "1. Videos that combine 'speed' and 'hasAdinText' decrease views (coef=-0.637584)\n",
      "2. Videos with 'korean' increase views (coef=0.718873)\n",
      "3. Videos that combine 'product' and 'prime_hour' increase views (coef=0.813412)\n",
      "4. Videos with 'popular_brand' increase views (coef=0.844806)\n",
      "5. Videos that combine 'prime_hour' and 'hasAdinText' decrease views (coef=-0.891425)\n",
      "6. Videos that combine 'popular_brand' and 'product' increase views (coef=0.990829)\n",
      "7. Videos that combine 'product' and 'speed' increase views (coef=1.126216)\n",
      "8. Videos that combine 'speed' and 'prime_hour' increase views (coef=1.426781)\n",
      "9. Videos that combine 'popular_brand' and 'prime_hour' increase views (coef=1.534968)\n",
      "10. Videos that combine 'popular_brand' and 'speed' increase views (coef=1.718733)\n"
     ]
    }
   ],
   "source": [
    "# Extract coefficients\n",
    "transformed_feature_names = pipe_linear.named_steps['interaction_terms'].get_feature_names_out(features)\n",
    "lasso_coeffs = pd.Series(\n",
    "    pipe_linear.named_steps['lasso'].coef_,\n",
    "    index=transformed_feature_names\n",
    ")\n",
    "\n",
    "# Get non-zero coefficients\n",
    "sig_lasso_coeffs = lasso_coeffs[lasso_coeffs != 0]\n",
    "important_features = sig_lasso_coeffs.sort_values(key=abs)\n",
    "\n",
    "print(\"\\nAll non-zero features sorted by coefficient magnitude:\")\n",
    "print(important_features)\n",
    "\n",
    "print(\"\\nTop 10 most important features:\")\n",
    "top_features = important_features.tail(10)\n",
    "print(top_features)\n",
    "\n",
    "# create hypotheses\n",
    "hypotheses = []\n",
    "for feature, coef in top_features.items():\n",
    "    expected_direction = \"positive\" if coef > 0 else \"negative\"\n",
    "    impact = \"increase\" if coef > 0 else \"decrease\"\n",
    "\n",
    "    if \" \" in feature:\n",
    "        # For interaction terms\n",
    "        parts = feature.split(\" \")\n",
    "        hypothesis_text = f\"Videos that combine '{parts[0]}' and '{parts[1]}' {impact} views\"\n",
    "    else:\n",
    "        # For single features\n",
    "        hypothesis_text = f\"Videos with '{feature}' {impact} views\"\n",
    "\n",
    "    hypotheses.append({\n",
    "        \"feature\": feature,\n",
    "        \"coefficient\": coef,\n",
    "        \"expected_direction\": expected_direction,\n",
    "        \"hypothesis\": hypothesis_text\n",
    "    })\n",
    "\n",
    "print(\"\\nFormulated hypotheses:\")\n",
    "for i, hyp in enumerate(hypotheses, 1):\n",
    "    print(f\"{i}. {hyp['hypothesis']} (coef={hyp['coefficient']:.6f})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'feature': 'speed hasAdinText',\n",
       "  'coefficient': -0.6375841563629866,\n",
       "  'expected_direction': 'negative',\n",
       "  'hypothesis': \"Videos that combine 'speed' and 'hasAdinText' decrease views\"},\n",
       " {'feature': 'korean',\n",
       "  'coefficient': 0.7188726902164926,\n",
       "  'expected_direction': 'positive',\n",
       "  'hypothesis': \"Videos with 'korean' increase views\"},\n",
       " {'feature': 'product prime_hour',\n",
       "  'coefficient': 0.8134122047713994,\n",
       "  'expected_direction': 'positive',\n",
       "  'hypothesis': \"Videos that combine 'product' and 'prime_hour' increase views\"},\n",
       " {'feature': 'popular_brand',\n",
       "  'coefficient': 0.8448064790479867,\n",
       "  'expected_direction': 'positive',\n",
       "  'hypothesis': \"Videos with 'popular_brand' increase views\"},\n",
       " {'feature': 'prime_hour hasAdinText',\n",
       "  'coefficient': -0.8914245652653439,\n",
       "  'expected_direction': 'negative',\n",
       "  'hypothesis': \"Videos that combine 'prime_hour' and 'hasAdinText' decrease views\"},\n",
       " {'feature': 'popular_brand product',\n",
       "  'coefficient': 0.9908293133774675,\n",
       "  'expected_direction': 'positive',\n",
       "  'hypothesis': \"Videos that combine 'popular_brand' and 'product' increase views\"},\n",
       " {'feature': 'product speed',\n",
       "  'coefficient': 1.1262157001107433,\n",
       "  'expected_direction': 'positive',\n",
       "  'hypothesis': \"Videos that combine 'product' and 'speed' increase views\"},\n",
       " {'feature': 'speed prime_hour',\n",
       "  'coefficient': 1.4267813713817523,\n",
       "  'expected_direction': 'positive',\n",
       "  'hypothesis': \"Videos that combine 'speed' and 'prime_hour' increase views\"},\n",
       " {'feature': 'popular_brand prime_hour',\n",
       "  'coefficient': 1.534968178757522,\n",
       "  'expected_direction': 'positive',\n",
       "  'hypothesis': \"Videos that combine 'popular_brand' and 'prime_hour' increase views\"},\n",
       " {'feature': 'popular_brand speed',\n",
       "  'coefficient': 1.7187332540226066,\n",
       "  'expected_direction': 'positive',\n",
       "  'hypothesis': \"Videos that combine 'popular_brand' and 'speed' increase views\"}]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Test each hypothesis on the confirmation dataset\n",
    "results = []\n",
    "\n",
    "for hypothesis in hypotheses:\n",
    "    feature = hypothesis[\"feature\"]\n",
    "    expected_direction = hypothesis[\"expected_direction\"]\n",
    "\n",
    "    if \" \" not in feature:\n",
    "        if feature not in df_confirm.columns:\n",
    "            results.append({\n",
    "                'feature': feature,\n",
    "                'hypothesis': hypothesis[\"hypothesis\"],\n",
    "                'result': \"Feature not found in dataset\",\n",
    "                'confirmed': False\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        # Group data based on feature presence\n",
    "        with_feature = df_confirm[df_confirm[feature] == 1][\"y\"]\n",
    "        without_feature = df_confirm[df_confirm[feature] == 0][\"y\"]\n",
    "\n",
    "        # Skip if either group is too small\n",
    "        if len(with_feature) < 10 or len(without_feature) < 10:\n",
    "            results.append({\n",
    "                'feature': feature,\n",
    "                'hypothesis': hypothesis[\"hypothesis\"],\n",
    "                'result': f\"Insufficient data (with={len(with_feature)}, without={len(without_feature)})\",\n",
    "                'confirmed': False\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        #  t-test\n",
    "        t_stat, p_value = stats.ttest_ind(with_feature, without_feature, equal_var=False)\n",
    "\n",
    "        # Check if result confirms hypothesis direction\n",
    "        mean_diff = with_feature.mean() - without_feature.mean()\n",
    "        direction_confirmed = (\n",
    "            (expected_direction == \"positive\" and mean_diff > 0) or\n",
    "            (expected_direction == \"negative\" and mean_diff < 0)\n",
    "        )\n",
    "\n",
    "        # Record results\n",
    "        results.append({\n",
    "            'feature': feature,\n",
    "            'hypothesis': hypothesis[\"hypothesis\"],\n",
    "            'mean_with': with_feature.mean(),\n",
    "            'mean_without': without_feature.mean(),\n",
    "            'difference': mean_diff,\n",
    "            'p_value': p_value,\n",
    "            'significant': p_value < 0.05,\n",
    "            't_statistic': t_stat,\n",
    "            'expected_direction': expected_direction,\n",
    "            'actual_direction': \"positive\" if mean_diff > 0 else \"negative\",\n",
    "            'direction_confirmed': direction_confirmed,\n",
    "            'hypothesis_confirmed': direction_confirmed and p_value < 0.05\n",
    "        })\n",
    "\n",
    "    # For interaction terms (simplified approach)\n",
    "    else:\n",
    "        feature_parts = feature.split(\" \")\n",
    "        if len(feature_parts) != 2:\n",
    "            results.append({\n",
    "                'feature': feature,\n",
    "                'hypothesis': hypothesis[\"hypothesis\"],\n",
    "                'result': \"Complex interaction term - not tested\",\n",
    "                'confirmed': False\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        feature1, feature2 = feature_parts\n",
    "\n",
    "        # Check if features exist\n",
    "        if feature1 not in df_confirm.columns or feature2 not in df_confirm.columns:\n",
    "            results.append({\n",
    "                'feature': feature,\n",
    "                'hypothesis': hypothesis[\"hypothesis\"],\n",
    "                'result': \"One or more features not found in dataset\",\n",
    "                'confirmed': False\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        # Create groups for interaction\n",
    "        both_present = df_confirm[(df_confirm[feature1] == 1) & (df_confirm[feature2] == 1)][\"y\"]\n",
    "        not_both = df_confirm[~((df_confirm[feature1] == 1) & (df_confirm[feature2] == 1))][\"y\"]\n",
    "\n",
    "        # Skip if either group is too small\n",
    "        if len(both_present) < 10 or len(not_both) < 10:\n",
    "            results.append({\n",
    "                'feature': feature,\n",
    "                'hypothesis': hypothesis[\"hypothesis\"],\n",
    "                'result': f\"Insufficient data (both={len(both_present)}, not_both={len(not_both)})\",\n",
    "                'confirmed': False\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        # Perform t-test\n",
    "        t_stat, p_value = stats.ttest_ind(both_present, not_both, equal_var=False)\n",
    "\n",
    "        # Check direction\n",
    "        mean_diff = both_present.mean() - not_both.mean()\n",
    "        direction_confirmed = (\n",
    "            (expected_direction == \"positive\" and mean_diff > 0) or\n",
    "            (expected_direction == \"negative\" and mean_diff < 0)\n",
    "        )\n",
    "\n",
    "        # Record results\n",
    "        results.append({\n",
    "            'feature': feature,\n",
    "            'hypothesis': hypothesis[\"hypothesis\"],\n",
    "            'mean_both': both_present.mean(),\n",
    "            'mean_not_both': not_both.mean(),\n",
    "            'difference': mean_diff,\n",
    "            'p_value': p_value,\n",
    "            'significant': p_value < 0.05,\n",
    "            't_statistic': t_stat,\n",
    "            'expected_direction': expected_direction,\n",
    "            'actual_direction': \"positive\" if mean_diff > 0 else \"negative\",\n",
    "            'direction_confirmed': direction_confirmed,\n",
    "            'hypothesis_confirmed': direction_confirmed and p_value < 0.05\n",
    "        })\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results Summary: 2 out of 10 hypotheses confirmed (20.0%)\n",
      "\n",
      "Confirmed Hypotheses:\n",
      "- Videos that combine 'speed' and 'hasAdinText' decrease views (p=0.0000)\n",
      "  Mean with speed hasAdinText: nan\n",
      "  Mean without speed hasAdinText: nan\n",
      "  Difference: -0.0118\n",
      "- Videos with 'korean' increase views (p=0.0099)\n",
      "  Mean with korean: 0.0597\n",
      "  Mean without korean: 0.0562\n",
      "  Difference: 0.0035\n",
      "\n",
      "Unconfirmed Hypotheses:\n",
      "- Videos that combine 'product' and 'prime_hour' increase views (p=0.0000)\n",
      "- Videos with 'popular_brand' increase views (p=0.0000)\n",
      "- Videos that combine 'prime_hour' and 'hasAdinText' decrease views (p=0.3188)\n",
      "- Videos that combine 'popular_brand' and 'product' increase views (p=0.0000)\n",
      "- Videos that combine 'product' and 'speed' increase views (p=0.7537)\n",
      "- Videos that combine 'speed' and 'prime_hour' increase views (p=0.8852)\n",
      "- Videos that combine 'popular_brand' and 'prime_hour' increase views (p=0.0079)\n",
      "- Videos that combine 'popular_brand' and 'speed' increase views (p=0.0006)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "confirmed_hypotheses = results_df[results_df['hypothesis_confirmed'] == True]\n",
    "confirmation_rate = len(confirmed_hypotheses) / len(results_df) * 100\n",
    "\n",
    "print(f\"\\nResults Summary: {len(confirmed_hypotheses)} out of {len(results_df)} hypotheses confirmed ({confirmation_rate:.1f}%)\")\n",
    "\n",
    "# confirmed hypotheses\n",
    "print(\"\\nConfirmed Hypotheses:\")\n",
    "for i, row in confirmed_hypotheses.iterrows():\n",
    "    if 'mean_with' in row:\n",
    "        print(f\"- {row['hypothesis']} (p={row['p_value']:.4f})\")\n",
    "        print(f\"  Mean with {row['feature']}: {row['mean_with']:.4f}\")\n",
    "        print(f\"  Mean without {row['feature']}: {row['mean_without']:.4f}\")\n",
    "        print(f\"  Difference: {row['difference']:.4f}\")\n",
    "    else:\n",
    "        print(f\"- {row['hypothesis']} (p={row['p_value']:.4f})\")\n",
    "        print(f\"  Mean with both features: {row['mean_both']:.4f}\")\n",
    "        print(f\"  Mean without both features: {row['mean_not_both']:.4f}\")\n",
    "        print(f\"  Difference: {row['difference']:.4f}\")\n",
    "# unconfirmed hypotheses\n",
    "unconfirmed = results_df[results_df['hypothesis_confirmed'] != True]\n",
    "print(\"\\nUnconfirmed Hypotheses:\")\n",
    "\n",
    "for i, row in unconfirmed.iterrows():\n",
    "    if 'p_value' in row and row['p_value'] is not None:\n",
    "        print(f\"- {row['hypothesis']} (p={row['p_value']:.4f})\")\n",
    "    else:\n",
    "        print(f\"- {row['hypothesis']} (Reason: {row['result']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
