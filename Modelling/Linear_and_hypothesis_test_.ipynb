{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "5vX0c4JOQYMp",
        "outputId": "8de64bd2-20af-4062-9b0f-b4dec05b1eae"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-2a389159-fe4d-4c8d-83d7-ccfc4074262e\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-2a389159-fe4d-4c8d-83d7-ccfc4074262e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving no_early_dates_all_features_train.csv to no_early_dates_all_features_train (2).csv\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "df = pd.read_csv('no_early_dates_all_features_train.csv')\n",
        "\n"
      ],
      "metadata": {
        "id": "rQlhGaw8QgYn"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1FAzACGrQvwy",
        "outputId": "bc5ea4fd-e147-47eb-b65b-c045bff81074"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Unnamed: 0', 'commentsCount', 'isChannelVerified', 'likes',\n",
              "       'numberOfSubscribers', 'text', 'title', 'viewCount',\n",
              "       'views_per_subscriber', 'duration_in_seconds', 'date',\n",
              "       'hashtag_indicator', 'has_any_affiliate', 'hasAdinTitle', 'hasAdinText',\n",
              "       'Engagement_per_Subscriber', 'Engagement_per_View', 'popular_brand',\n",
              "       'prime_hour', 'product', 'skills/teach', 'speed', 'comparing_products',\n",
              "       'self_ref', 'budget', 'korean'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "features = [\"popular_brand\", \"has_any_affiliate\", \"product\", \"budget\", \"self_ref\", \"korean\", \"speed\", \"skills/teach\", \"comparing_products\", \"prime_hour\", \"hasAdinTitle\", \"hasAdinText\",'hashtag_indicator']\n",
        "\n",
        "#Create the target column $y$ here\n",
        "df[\"y\"] = (df[\"likes\"] + df[\"commentsCount\"])  / (df[\"viewCount\"] + 1)\n",
        "\n",
        "#get rid of noisy columns\n",
        "df = df[ features + [\"y\"] ]"
      ],
      "metadata": {
        "id": "smyHNMO3VAVn"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split data into exploration set and confirmation\n",
        "df_explore, df_confirm = train_test_split(df, test_size=0.5, random_state=42)\n",
        "\n",
        "print(f\"Exploration set: {df_explore.shape[0]} rows\")\n",
        "print(f\"Confirmation set: {df_confirm.shape[0]} rows\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JorvUfeNQv53",
        "outputId": "46ab5c78-c5a2-4352-e1b5-8a47a7fcdc0f"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exploration set: 3968 rows\n",
            "Confirmation set: 3969 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LinearRegression, Lasso\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_explore_scaled = scaler.fit_transform(df_explore[features])\n",
        "\n",
        "pipe_linear = Pipeline([\n",
        "    (\"interaction_terms\", PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)),\n",
        "    (\"lasso\", Lasso(alpha=0.0001, max_iter=10000))\n",
        "])\n",
        "pipe_linear.fit(X_explore_scaled, df_explore[\"y\"])\n",
        "\n",
        "linear_pred = pipe_linear.predict(X_explore_scaled)\n",
        "linear_mse = mean_squared_error(df_explore[\"y\"], linear_pred)\n",
        "\n",
        "\n",
        "#RF\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf.fit(X_explore_scaled, df_explore[\"y\"])\n",
        "rf_pred = rf.predict(X_explore_scaled)\n",
        "rf_mse = mean_squared_error(df_explore[\"y\"], rf_pred)\n",
        "\n",
        "# Calculate performance ratio on real data\n",
        "real_ratio = linear_mse / rf_mse\n",
        "\n",
        "print(\"Performance on real data:\")\n",
        "print(f\"Linear model MSE: {linear_mse:.6f}\")\n",
        "print(f\"Random Forest MSE: {rf_mse:.6f}\")\n",
        "print(f\"Improvement ratio: {real_ratio:.2f}x (how much better RF is than linear)\")\n",
        "\n",
        "\n",
        "# Generate simulated data under hyphothesis 0\n",
        "residuals = df_explore[\"y\"] - linear_pred\n",
        "std_residuals = np.std(residuals)\n",
        "np.random.seed(42)\n",
        "y_simulated = linear_pred + np.random.normal(0, std_residuals, len(linear_pred))\n",
        "\n",
        "pipe_linear_sim = Pipeline([\n",
        "    (\"interaction_terms\", PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)),\n",
        "    (\"lasso\", Lasso(alpha=0.0001, max_iter=10000))\n",
        "])\n",
        "pipe_linear_sim.fit(X_explore_scaled, y_simulated)\n",
        "linear_sim_pred = pipe_linear_sim.predict(X_explore_scaled)\n",
        "linear_sim_mse = mean_squared_error(y_simulated, linear_sim_pred)\n",
        "\n",
        "rf_sim = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_sim.fit(X_explore_scaled, y_simulated)\n",
        "rf_sim_pred = rf_sim.predict(X_explore_scaled)\n",
        "rf_sim_mse = mean_squared_error(y_simulated, rf_sim_pred)\n",
        "\n",
        "sim_ratio = linear_sim_mse / rf_sim_mse\n",
        "\n",
        "print(\"\\nPerformance on simulated linear data:\")\n",
        "print(f\"Linear model MSE: {linear_sim_mse:.6f}\")\n",
        "print(f\"Random Forest MSE: {rf_sim_mse:.6f}\")\n",
        "print(f\"Improvement ratio: {sim_ratio:.2f}x (how much better RF is than linear)\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NWWP9t0WQwAI",
        "outputId": "10e186c3-f06d-442f-c91d-9c7ed2c8c614"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performance on real data:\n",
            "Linear model MSE: 0.000999\n",
            "Random Forest MSE: 0.000784\n",
            "Improvement ratio: 1.27x (how much better RF is than linear)\n",
            "\n",
            "Performance on simulated linear data:\n",
            "Linear model MSE: 0.000970\n",
            "Random Forest MSE: 0.000801\n",
            "Improvement ratio: 1.21x (how much better RF is than linear)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract coefficients\n",
        "transformed_feature_names = pipe_linear.named_steps['interaction_terms'].get_feature_names_out(features)\n",
        "lasso_coeffs = pd.Series(\n",
        "    pipe_linear.named_steps['lasso'].coef_,\n",
        "    index=transformed_feature_names\n",
        ")\n",
        "\n",
        "# Get non-zero coefficients\n",
        "sig_lasso_coeffs = lasso_coeffs[lasso_coeffs != 0]\n",
        "important_features = sig_lasso_coeffs.sort_values(key=abs)\n",
        "\n",
        "print(\"\\nAll non-zero features sorted by coefficient magnitude:\")\n",
        "print(important_features)\n",
        "\n",
        "print(\"\\nTop 10 most important features:\")\n",
        "top_features = important_features.tail(10)\n",
        "print(top_features)\n",
        "\n",
        "# create hypotheses\n",
        "hypotheses = []\n",
        "for feature, coef in top_features.items():\n",
        "    expected_direction = \"positive\" if coef > 0 else \"negative\"\n",
        "    impact = \"increase\" if coef > 0 else \"decrease\"\n",
        "\n",
        "    if \" \" in feature:\n",
        "        # For interaction terms\n",
        "        parts = feature.split(\" \")\n",
        "        hypothesis_text = f\"Videos that combine '{parts[0]}' and '{parts[1]}' {impact} engagement\"\n",
        "    else:\n",
        "        # For single features\n",
        "        hypothesis_text = f\"Videos with '{feature}' {impact} engagement\"\n",
        "\n",
        "    hypotheses.append({\n",
        "        \"feature\": feature,\n",
        "        \"coefficient\": coef,\n",
        "        \"expected_direction\": expected_direction,\n",
        "        \"hypothesis\": hypothesis_text\n",
        "    })\n",
        "\n",
        "print(\"\\nFormulated hypotheses:\")\n",
        "for i, hyp in enumerate(hypotheses, 1):\n",
        "    print(f\"{i}. {hyp['hypothesis']} (coef={hyp['coefficient']:.6f})\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XM09X92AQwDI",
        "outputId": "23eb76bd-6bcf-4b72-a388-fc71dbae423d"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "All non-zero features sorted by coefficient magnitude:\n",
            "skills/teach hasAdinText       -0.000013\n",
            "budget comparing_products      -0.000022\n",
            "prime_hour hasAdinTitle         0.000033\n",
            "prime_hour hashtag_indicator    0.000035\n",
            "popular_brand budget            0.000044\n",
            "                                  ...   \n",
            "prime_hour                      0.002429\n",
            "product hashtag_indicator       0.002747\n",
            "hashtag_indicator              -0.003010\n",
            "popular_brand skills/teach      0.003132\n",
            "popular_brand                  -0.003172\n",
            "Length: 75, dtype: float64\n",
            "\n",
            "Top 10 most important features:\n",
            "skills/teach                       0.001843\n",
            "self_ref                           0.001880\n",
            "popular_brand hashtag_indicator   -0.001995\n",
            "skills/teach hashtag_indicator    -0.002008\n",
            "self_ref speed                    -0.002370\n",
            "prime_hour                         0.002429\n",
            "product hashtag_indicator          0.002747\n",
            "hashtag_indicator                 -0.003010\n",
            "popular_brand skills/teach         0.003132\n",
            "popular_brand                     -0.003172\n",
            "dtype: float64\n",
            "\n",
            "Formulated hypotheses:\n",
            "1. Videos with 'skills/teach' increase engagement (coef=0.001843)\n",
            "2. Videos with 'self_ref' increase engagement (coef=0.001880)\n",
            "3. Videos that combine 'popular_brand' and 'hashtag_indicator' decrease engagement (coef=-0.001995)\n",
            "4. Videos that combine 'skills/teach' and 'hashtag_indicator' decrease engagement (coef=-0.002008)\n",
            "5. Videos that combine 'self_ref' and 'speed' decrease engagement (coef=-0.002370)\n",
            "6. Videos with 'prime_hour' increase engagement (coef=0.002429)\n",
            "7. Videos that combine 'product' and 'hashtag_indicator' increase engagement (coef=0.002747)\n",
            "8. Videos with 'hashtag_indicator' decrease engagement (coef=-0.003010)\n",
            "9. Videos that combine 'popular_brand' and 'skills/teach' increase engagement (coef=0.003132)\n",
            "10. Videos with 'popular_brand' decrease engagement (coef=-0.003172)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_confirm.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jBDxCjUKHRXt",
        "outputId": "dffa0f7b-e783-4d3f-faf3-f8c997b78479"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['popular_brand', 'has_any_affiliate', 'product', 'budget', 'self_ref',\n",
              "       'korean', 'speed', 'skills/teach', 'comparing_products', 'prime_hour',\n",
              "       'hasAdinTitle', 'hasAdinText', 'hashtag_indicator', 'y'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hypotheses"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iX4jl0T7C7z5",
        "outputId": "fd36ad7b-68aa-483a-b9c7-b3cef2c703b1"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'feature': 'skills/teach',\n",
              "  'coefficient': 0.001843409523709859,\n",
              "  'expected_direction': 'positive',\n",
              "  'hypothesis': \"Videos with 'skills/teach' increase engagement\"},\n",
              " {'feature': 'self_ref',\n",
              "  'coefficient': 0.0018799266533280512,\n",
              "  'expected_direction': 'positive',\n",
              "  'hypothesis': \"Videos with 'self_ref' increase engagement\"},\n",
              " {'feature': 'popular_brand hashtag_indicator',\n",
              "  'coefficient': -0.0019954248224198215,\n",
              "  'expected_direction': 'negative',\n",
              "  'hypothesis': \"Videos that combine 'popular_brand' and 'hashtag_indicator' decrease engagement\"},\n",
              " {'feature': 'skills/teach hashtag_indicator',\n",
              "  'coefficient': -0.00200766036931156,\n",
              "  'expected_direction': 'negative',\n",
              "  'hypothesis': \"Videos that combine 'skills/teach' and 'hashtag_indicator' decrease engagement\"},\n",
              " {'feature': 'self_ref speed',\n",
              "  'coefficient': -0.0023700812120385456,\n",
              "  'expected_direction': 'negative',\n",
              "  'hypothesis': \"Videos that combine 'self_ref' and 'speed' decrease engagement\"},\n",
              " {'feature': 'prime_hour',\n",
              "  'coefficient': 0.0024293714883996706,\n",
              "  'expected_direction': 'positive',\n",
              "  'hypothesis': \"Videos with 'prime_hour' increase engagement\"},\n",
              " {'feature': 'product hashtag_indicator',\n",
              "  'coefficient': 0.002747104620766307,\n",
              "  'expected_direction': 'positive',\n",
              "  'hypothesis': \"Videos that combine 'product' and 'hashtag_indicator' increase engagement\"},\n",
              " {'feature': 'hashtag_indicator',\n",
              "  'coefficient': -0.0030104652853345437,\n",
              "  'expected_direction': 'negative',\n",
              "  'hypothesis': \"Videos with 'hashtag_indicator' decrease engagement\"},\n",
              " {'feature': 'popular_brand skills/teach',\n",
              "  'coefficient': 0.003132088420966795,\n",
              "  'expected_direction': 'positive',\n",
              "  'hypothesis': \"Videos that combine 'popular_brand' and 'skills/teach' increase engagement\"},\n",
              " {'feature': 'popular_brand',\n",
              "  'coefficient': -0.003172133677226053,\n",
              "  'expected_direction': 'negative',\n",
              "  'hypothesis': \"Videos with 'popular_brand' decrease engagement\"}]"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from scipy import stats\n",
        "\n",
        "# Test each hypothesis on the confirmation dataset\n",
        "results = []\n",
        "\n",
        "for hypothesis in hypotheses:\n",
        "    feature = hypothesis[\"feature\"]\n",
        "    expected_direction = hypothesis[\"expected_direction\"]\n",
        "\n",
        "    if \" \" not in feature:\n",
        "        if feature not in df_confirm.columns:\n",
        "            results.append({\n",
        "                'feature': feature,\n",
        "                'hypothesis': hypothesis[\"hypothesis\"],\n",
        "                'result': \"Feature not found in dataset\",\n",
        "                'confirmed': False\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        # Group data based on feature presence\n",
        "        with_feature = df_confirm[df_confirm[feature] == 1][\"y\"]\n",
        "        without_feature = df_confirm[df_confirm[feature] == 0][\"y\"]\n",
        "\n",
        "        # Skip if either group is too small\n",
        "        if len(with_feature) < 10 or len(without_feature) < 10:\n",
        "            results.append({\n",
        "                'feature': feature,\n",
        "                'hypothesis': hypothesis[\"hypothesis\"],\n",
        "                'result': f\"Insufficient data (with={len(with_feature)}, without={len(without_feature)})\",\n",
        "                'confirmed': False\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        #  t-test\n",
        "        t_stat, p_value = stats.ttest_ind(with_feature, without_feature, equal_var=False)\n",
        "\n",
        "        # Check if result confirms hypothesis direction\n",
        "        mean_diff = with_feature.mean() - without_feature.mean()\n",
        "        direction_confirmed = (\n",
        "            (expected_direction == \"positive\" and mean_diff > 0) or\n",
        "            (expected_direction == \"negative\" and mean_diff < 0)\n",
        "        )\n",
        "\n",
        "        # Record results\n",
        "        results.append({\n",
        "            'feature': feature,\n",
        "            'hypothesis': hypothesis[\"hypothesis\"],\n",
        "            'mean_with': with_feature.mean(),\n",
        "            'mean_without': without_feature.mean(),\n",
        "            'difference': mean_diff,\n",
        "            'p_value': p_value,\n",
        "            'significant': p_value < 0.05,\n",
        "            't_statistic': t_stat,\n",
        "            'expected_direction': expected_direction,\n",
        "            'actual_direction': \"positive\" if mean_diff > 0 else \"negative\",\n",
        "            'direction_confirmed': direction_confirmed,\n",
        "            'hypothesis_confirmed': direction_confirmed and p_value < 0.05\n",
        "        })\n",
        "\n",
        "    # For interaction terms (simplified approach)\n",
        "    else:\n",
        "        feature_parts = feature.split(\" \")\n",
        "        if len(feature_parts) != 2:\n",
        "            results.append({\n",
        "                'feature': feature,\n",
        "                'hypothesis': hypothesis[\"hypothesis\"],\n",
        "                'result': \"Complex interaction term - not tested\",\n",
        "                'confirmed': False\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        feature1, feature2 = feature_parts\n",
        "\n",
        "        # Check if features exist\n",
        "        if feature1 not in df_confirm.columns or feature2 not in df_confirm.columns:\n",
        "            results.append({\n",
        "                'feature': feature,\n",
        "                'hypothesis': hypothesis[\"hypothesis\"],\n",
        "                'result': \"One or more features not found in dataset\",\n",
        "                'confirmed': False\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        # Create groups for interaction\n",
        "        both_present = df_confirm[(df_confirm[feature1] == 1) & (df_confirm[feature2] == 1)][\"y\"]\n",
        "        not_both = df_confirm[~((df_confirm[feature1] == 1) & (df_confirm[feature2] == 1))][\"y\"]\n",
        "\n",
        "        # Skip if either group is too small\n",
        "        if len(both_present) < 10 or len(not_both) < 10:\n",
        "            results.append({\n",
        "                'feature': feature,\n",
        "                'hypothesis': hypothesis[\"hypothesis\"],\n",
        "                'result': f\"Insufficient data (both={len(both_present)}, not_both={len(not_both)})\",\n",
        "                'confirmed': False\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        # Perform t-test\n",
        "        t_stat, p_value = stats.ttest_ind(both_present, not_both, equal_var=False)\n",
        "\n",
        "        # Check direction\n",
        "        mean_diff = both_present.mean() - not_both.mean()\n",
        "        direction_confirmed = (\n",
        "            (expected_direction == \"positive\" and mean_diff > 0) or\n",
        "            (expected_direction == \"negative\" and mean_diff < 0)\n",
        "        )\n",
        "\n",
        "        # Record results\n",
        "        results.append({\n",
        "            'feature': feature,\n",
        "            'hypothesis': hypothesis[\"hypothesis\"],\n",
        "            'mean_both': both_present.mean(),\n",
        "            'mean_not_both': not_both.mean(),\n",
        "            'difference': mean_diff,\n",
        "            'p_value': p_value,\n",
        "            'significant': p_value < 0.05,\n",
        "            't_statistic': t_stat,\n",
        "            'expected_direction': expected_direction,\n",
        "            'actual_direction': \"positive\" if mean_diff > 0 else \"negative\",\n",
        "            'direction_confirmed': direction_confirmed,\n",
        "            'hypothesis_confirmed': direction_confirmed and p_value < 0.05\n",
        "        })\n",
        "\n",
        "\n",
        "results_df = pd.DataFrame(results)"
      ],
      "metadata": {
        "id": "-X_hRFq_d4sb"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7GQQCOkMtiM",
        "outputId": "3ba9dc6f-8740-4b66-ee69-d25d8e303b7f"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 14)"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "confirmed_hypotheses = results_df[results_df['hypothesis_confirmed'] == True]\n",
        "confirmation_rate = len(confirmed_hypotheses) / len(results_df) * 100\n",
        "\n",
        "print(f\"\\nResults Summary: {len(confirmed_hypotheses)} out of {len(results_df)} hypotheses confirmed ({confirmation_rate:.1f}%)\")\n",
        "\n",
        "# confirmed hypotheses\n",
        "print(\"\\nConfirmed Hypotheses:\")\n",
        "for i, row in confirmed_hypotheses.iterrows():\n",
        "    if 'mean_with' in row:\n",
        "        print(f\"- {row['hypothesis']} (p={row['p_value']:.4f})\")\n",
        "        print(f\"  Mean with {row['feature']}: {row['mean_with']:.4f}\")\n",
        "        print(f\"  Mean without {row['feature']}: {row['mean_without']:.4f}\")\n",
        "        print(f\"  Difference: {row['difference']:.4f}\")\n",
        "    else:\n",
        "        print(f\"- {row['hypothesis']} (p={row['p_value']:.4f})\")\n",
        "        print(f\"  Mean with both features: {row['mean_both']:.4f}\")\n",
        "        print(f\"  Mean without both features: {row['mean_not_both']:.4f}\")\n",
        "        print(f\"  Difference: {row['difference']:.4f}\")\n",
        "# unconfirmed hypotheses\n",
        "unconfirmed = results_df[results_df['hypothesis_confirmed'] != True]\n",
        "print(\"\\nUnconfirmed Hypotheses:\")\n",
        "\n",
        "for i, row in unconfirmed.iterrows():\n",
        "    if 'p_value' in row and row['p_value'] is not None:\n",
        "        print(f\"- {row['hypothesis']} (p={row['p_value']:.4f})\")\n",
        "    else:\n",
        "        print(f\"- {row['hypothesis']} (Reason: {row['result']})\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hWODFw-8d4wM",
        "outputId": "8929cef4-3be2-4b87-f348-ba454ae786aa"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Results Summary: 4 out of 10 hypotheses confirmed (40.0%)\n",
            "\n",
            "Confirmed Hypotheses:\n",
            "- Videos with 'self_ref' increase engagement (p=0.0005)\n",
            "  Mean with self_ref: 0.0581\n",
            "  Mean without self_ref: 0.0544\n",
            "  Difference: 0.0037\n",
            "- Videos with 'prime_hour' increase engagement (p=0.0007)\n",
            "  Mean with prime_hour: 0.0581\n",
            "  Mean without prime_hour: 0.0545\n",
            "  Difference: 0.0036\n",
            "- Videos with 'hashtag_indicator' decrease engagement (p=0.0001)\n",
            "  Mean with hashtag_indicator: 0.0527\n",
            "  Mean without hashtag_indicator: 0.0584\n",
            "  Difference: -0.0057\n",
            "- Videos with 'popular_brand' decrease engagement (p=0.0000)\n",
            "  Mean with popular_brand: 0.0489\n",
            "  Mean without popular_brand: 0.0575\n",
            "  Difference: -0.0087\n",
            "\n",
            "Unconfirmed Hypotheses:\n",
            "- Videos with 'skills/teach' increase engagement (p=0.0637)\n",
            "- Videos that combine 'popular_brand' and 'hashtag_indicator' decrease engagement (p=0.2639)\n",
            "- Videos that combine 'skills/teach' and 'hashtag_indicator' decrease engagement (p=0.5387)\n",
            "- Videos that combine 'self_ref' and 'speed' decrease engagement (p=0.2429)\n",
            "- Videos that combine 'product' and 'hashtag_indicator' increase engagement (p=0.0004)\n",
            "- Videos that combine 'popular_brand' and 'skills/teach' increase engagement (p=0.4878)\n",
            "\n",
            "Our model specification testing supports the use of linear models for understanding these relationships.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v7Am1VHBd40L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}